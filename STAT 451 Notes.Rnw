\documentclass[12pt,letterpaper,oneside]{article}
\newcommand{\university}{Brigham Young University}
\newcommand{\classCode}{STAT 451}
\newcommand{\className}{Applied Bayesian Statistics}
\newcommand{\documentName}{Notes}
\newcommand{\dateTaken}{January 7, 2013}
\newcommand{\semester}{Winter 2013}
\newcommand{\teacher}{Dr. Gil Fellingham}
\newcommand{\docVersion}{00000}
\newcommand{\authorName}{\href{mailto:adam@acjackman.com}{Adam Jackman}}

\input{./Styles/code.tex}
\input{./Styles/pageStyle.tex}

\input{./Styles/mathCommands.tex}

<<KnitrSettings, cache=FALSE, include=FALSE>>=
# Adam Jackman
# December 3, 2012
opts_chunk$set(fig.path='figure/graphic-', cache.path='cache/graphic-', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', out.width='.4\\textwidth', cache=TRUE, par=TRUE, size="scriptsize")
options(width=80)
knit_hooks$set(crop = hook_pdfcrop)
# render_listings() # Use listings instead of regular output
library(R2jags)
inter_plots <- FALSE
@

\begin{document}
\input{./Styles/title.tex}
\tableofcontents
\renewcommand{\dateTaken}{January 8, 2013}
\daysep

\section{Class Introduction} % (fold)
\label{sec:class_introduction}
Office Hours Monday

Keep a copy of the code

Preliminary assignment email now.
% section class_introduction (end)

\section{Bayesian Methodology} % (fold)
\label{sec:bayesian_methodology}
Parameterize state of knowledge

Estimate parameters

\begin{align*}
    f(\underline{\theta}|\underline{x})=\frac{L(\underline{x}|\underline{\theta})\prod(\underline{\theta})}{\iint_{\underline{\theta}} L(\underline{x}|\underline{\theta})\prod(\underline{\theta}) \dd \underline{\theta}} \propto L(\underline{x}| \underline{\theta})\prod(\underline{\theta})
\end{align*}
\subsection{Metropolis Sampler} % (fold)
\label{sub:metropolis_sampler}

\begin{align*}
    \pi\left[\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1} \exp\left( -\frac{x}{\beta}\right)\right]+ (\pi-1)\left[\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1} \exp\left( -\frac{x}{\beta}\right)\right]\\
    \frac{1}{4}\left[\frac{1}{\Gamma(2)3^{2}}x^{2-1} \exp\left( -\frac{x}{3}\right)\right] +
    \frac{3}{4}\left[\frac{1}{\Gamma(4)3^{4}}x^{4-1} \exp\left( -\frac{x}{3}\right)\right]\\
    k=4\Gamma(4) 3^3
\end{align*}
\begin{align*}
    g(x)=k \cdot f(x)= 18 x \exp\left(-\frac{x}{3}\right)+ x^3\exp\left(-\frac{x}{3}\right)
\end{align*}
\subsubsection{Metropolis Sampler Steps} % (fold)
\label{ssub:metropolis_sampler_steps}
\begin{itemize}
    \item Initial $x=x_0$
    \item for $i=1$ to $n$
    \begin{itemize}
        \item generate candidate $c$
        \item $\displaystyle{\frac{g(c)}{g(x_{i-1}} = r}$
        \item let $x_i=c$ with $\prob=\min(1,r)$
    \end{itemize}
\end{itemize}
% subsubsection metropolis_sampler_steps (end)
\subsubsection{Metropolis Code} % (fold)
\label{ssub:metropolis_code}
<<metropolisSampler,cache=TRUE>>=
# Define g function
g <- function(x){
18 * x * exp(-x/3) + x^3 * exp(-x/3)
}

# Write Sampler
out <- NULL
out[1] <- 5
candidate.sigma <- 20
counter <- 0
samples <- 10^4
for (i in 2:samples) {
    out[i] <- out[i - 1]
    candidate <- rnorm(1, out[i-1], candidate.sigma)
    if (candidate > 0){
        r <- g(candidate)/g(out[i-1])
        if (r > runif(1,0,1)){
            out[i] <- candidate
            counter <- counter +1
        }
    }
}
message(counter/samples) # should be about 24-40
plot(out, type='l', main="Trace Plot")
@
Trace Plot looks good

<<MetropolisSamplerPosteriorPlot>>=
xx <- seq(0,50,length=100000)
k <- 4*gamma(4)*3^3
# Plot the exact distribution
plot(xx,g(xx)/k, lwd=2, col='blue')
# curve(.25*dgamma(x, shape=2, scale=3)+.75*dgamma(x, shape=4, scale=3), add=TRUE, col='blue')
# Both of these should produce the same line

lines(density(out),lwd=2)

# Exact Mean
.25*(2*3)+.75*(4*3)

# Approximate Mean
mean(out)
@
% subsubsection metropolis_code (end)
% subsection metropolis_sampler (end)
% section bayesian_methodology (end)
\renewcommand{\dateTaken}{January 10, 2013}
\daysep

Paul Sabin --- Stat 451 TA\\
Tuesday 2:30-4:00\\
Friday: 9:30-11:00\\
Office: 233\\
Email:\href{mailto:r.paul.sabin@gmail.com}{r.paul.sabin@gmail.com}

<<eval=FALSE>>=
# For Winbugs
install.packages("ARM", "R2WinBUGS")

# For jags
install.packages("R2jags")
@

\section{Two Sample $t$ test} % (fold)
\label{sec:two_sample_t_test}

\begin{align*}
y \sim \mathcal{N}(\mu, \sigma^2)\quad i=1,2\\
f(\mu_1)=\mathcal{N}(\mu=0, \sigma^2=1000)
f(\sigma^2)=\text{Unif}(0, 5000)
\end{align*}
% section two_sample_t_test (end)

PROC MCMC two group $t$ test
\begin{lstlisting}[language=SAS]
data examp2;
infile dataset;
input tmt response;
run;

/*
nmc: Number of Marcov Chains
nbi: Number Burn in

dic: information criteria
*/
proc mcmc data=examp2 outpost=post2 seed=1234 nmc=100000 nbi=5000
                      statistics(alpha=.05)=(summary interval) thin=10
                      moniter=( parms mudif varratio) propcov=quannew
                      diagnostics=(all) dic ;
array mu[2] mu1-mu2;
arra sig2[2] sig21-sig22;
prams: mu: sig2:;
prior mu: ~ normal(0, sd=10000);
prior sig21 ~ unif(9, 5000); /*  gamma(shape=30, scale=50) */
prior sig22 ~ unif(9, 5000);
mudif = mu1 - mu2;
varratio = sig21/sig22;
mm = mu[tmt];
vv = sig2[tmt];
model response ~ normal(mm, var=vv);
run;

proc export data=post2 outfile='twogroups.csv' dbms=csv replace;
run;
\end{lstlisting}

\begin{description}
    \item[HPD --- Highest Posterior Density] the smallest interval
\end{description}

Fischer variance problem --- a two sample $t$ test with unequal variances


% <<engine='SAS'>>=
% data foo;
% input a b c;
% datalines;
% 1 2 3
% 4 5 6
% 7 8 9
% ;
% run;
% @

Check the posterior autocorolations

\renewcommand{\dateTaken}{January 15, 2013}
\daysep
<<eval=FALSE>>=
    library(rjags)
    library(R2jags)
@
JAGS uses precisions, not variances. Precisions are $\frac{1}{\sigma^2}$

\lstinputlisting[language=R]{code/TwoSampleJAGS.R}


\renewcommand{\dateTaken}{January 17, 2013}
\daysep

You need to think about priors

\section{Diagnostics} % (fold)
\label{sec:diagnostics}
<<>>=
library(R2jags)
mdl <- "
    model {
        for (i in 1:33) {
            y[i] ~ dnorm(mu[tmt[i]], prec[tmt[i]])
        }

        for (i in 1:2) {
            mu[i] ~ dnorm(0,0.000001)
            vr[i] ~ dunif(0,5000)
            prec[i] <- 1/vr[i]
        }
    }
"
groups <- read.table("data/01twogroups.dat", col.names=c("tmt", "y"))
writeLines(mdl, "code/twogroups.jags")
tmt <- groups$tmt
y <- groups$y
# Data to go into jags
data.jags <- c('tmt','y')
# what parameters to keep track of
parms <- c('mu','vr')
# Initial Values
innts <- function() {list('mu' = rnorm(2,125,5), 'vr' = runif(2,0,5000))}
twogroups.sim <- jags(data=data.jags, parameters.to.save=parms, inits=innts, model.file="code/twogroups.jags",
    n.iter=6000, n.burnin=1000, n.chains=1, n.thin=1)

twogroups.sim

sims <- as.mcmc(twogroups.sim)
plot(sims, auto.layout=FALSE, ask=FALSE)

# plot(sims[,2], type='l')
@

First diagnostic: Look at your trace plot

Second diagnostic: look at auto corilation
<<>>=
autocorr(sims)
autocorr.plot(sims)
@

A good autocorrelation plot shows the first bar being tall, but the rest being relatively small.

Fourth diagnostic: Effective Sample size
<<>>=
effectiveSize(sims)
@
these should be relatively close to the actual sample size

fifth diagnostic: Raftery Lewis Diagnostic
<<>>=
raftery.diag(sims)
@
Check if you are within .005 (in the tails) with high probability

Dependence Factor should be less than 5

These five are the most commonly used, but there are others
<<>>=
geweke.diag(sims)
@
Tests for drift in the samples. does something like a $t$ test on the first 10\% and the last 50\% of the chain.

Heild
<<>>=
heidel.diag(sims)
@
test of stationarity, or convergence

both the stationarity and half width mean test should pass.


Gelmen needs chains started at different points
% section diagnostics (end)
\renewcommand{\dateTaken}{January 22, 2013}
\daysep
\section{Posterior Predictives} % (fold)
\label{sec:posterior_sredictives}
Liklihood $g_{i1}\sim \mathcal{N}(\mu_1, \sigma^2_1$, $g_{i2}\sim \mathcal{N}(\mu_2, \sigma^2_2$

Get a new y by plugging the sampled values of the parameters into a random sampler for the likelihood
\begin{lstlisting}
ynew_1 <- rnorm(1,mu1[1],sigma1[1])
\end{lstlisting}

\begin{tabular}{ccccc}
 $\mu_1$ & $\sigma^2_1$ & $\mu_2$ & $\sigma^2_2$ & $y_{\text{new}1}$\\
 \vdots & \vdots & \vdots & \vdots & \vdots
\end{tabular}

<<>>=
ratios <- read.table("../Homework/01/monkeyratio.dat")[,1]
library(R2jags)
N <- length(ratios)

mdl <- "
    model {
        for (i in 1:N) {
            ratios[i] ~ dbeta(alpha, beta)
        }

        alpha ~ dgamma(5, .025)
        beta ~ dgamma(5, .025)
    }
"
writeLines(mdl, "code/hw1.jags")

jags.data <- c("ratios", "N")
jags.params <- c("alpha", "beta")
jags.inits <- function() {
    list('alpha' <- rgamma(5,.025), 'beta' <- rgamma(5,.025))
}
set.seed(5487)

ratios.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   # inits=jags.inits,
                   model.file="code/hw1.jags",
                   n.iter=6000, n.burnin=1000, n.chains=1, n.thin=1)

sims <- as.mcmc(ratios.sim)

# autocorr.plot(sims)
alpha <- sims[,1]
beta <- sims[,2]
ynew <- rbeta(10000,alpha,beta)

# Plot of the density of the mean and the posterior predictive
plot(density(alpha/(alpha+beta)), xlim=c(.6,.9))
lines(density(ynew), col='blue') # Plot of the posterior predictive
@

 We don't make up data we get it from ``Pedagogically flexible data acquisition.''---Dr Tolly.


You can use other priors, they just may not be as easy for example:
<<include=FALSE>>=
mdl <- '
model {
    for (i in 1:5) {
        y[i] ~ dnorm(mu, prec)
    }

    mu ~ dunif(0,1)
    s2 ~ dunif(0,.05)
    prec <- 1/s2
}
'
writeLines(mdl,"code/hw1norm.jags")
y <- ratios
jags.data <- 'y'
jags.params <- c('mu','s2')
hw1norm.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   model.file="code/hw1norm.jags",
                   n.iter=6000, n.burnin=1000, n.chains=1, n.thin=1)
sims <- as.mcmc(hw1norm.sim)
plot(sims, auto.layout=FALSE, ask=FALSE)

@
% section posterior_predictives (end)
\section{Linear Regression} % (fold)
\label{sec:linear_regression}
Likelihood \[ y_{i} ~ \mathcal{N}(\mu_{i}, \sigma^2)\]

The mean is different for every data point, but the variance is constant.

\begin{align*}
    \mu_1 \leftarrow \beta_{0} + \beta_{1} x\\
    \beta_{0} \sim \mathcal{N}(,\tau=.001)\\
    \beta_{1} \sim \mathcal{N}(\mu=0,\tau=.001)\\
    \sigma^{2} \sim \\
\end{align*}
\renewcommand{\dateTaken}{January 24, 2013}
\daysep

<<>>=
jags.modelfile <- 'code/linreg.jags'
mdl <- '
    model {
        for (i in 1:N) {
            y[i] ~ dnorm(mu[i], prec)
            mu[i] <- beta_0 + beta_1 * x[i]
        }

        beta_0 ~ dnorm(0, .00001)
        beta_1 ~ dnorm(0, .001)
        sigma2 ~ dgamma(1.1, .5)
        prec <- 1/sigma2
    }
'
writeLines(mdl, 'code/linreg.jags')


linregdata <- read.table("data/02linreg.dat", col.names=c('lotsize', 'manhours'))
N <- nrow(linregdata)
y <- linregdata[,2]
x <- linregdata[,1]
jags.data <- c('y', 'x', 'N')
jags.params <- c('beta_0', 'beta_1', 'sigma2')


set.seed(5487)
linreg.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   model.file='code/linreg.jags',
                   n.iter=10000, n.burnin=5000, n.chains=1, n.thin=1)
linreg.sim
@

Let's check for convergence
<<>>=
sims <- as.mcmc(linreg.sim)
# Trace Plots
plot(sims, auto.layout=FALSE, ask=FALSE)
# Autocorrelation plots
autocorr.plot(sims, ask=FALSE)
effectiveSize(sims)
raftery.diag(sims)
@

Here's the frequentest method for an ANOVA
<<>>=
fit.1 <- lm(y~x)
anova(fit.1)
@

to check the value of the slope
<<>>=
beta0 <- sims[,1]
beta1 <- sims[,2]
sigma2 <- sims[,4]

# Probability slope is greater than 0
mean(beta1>0)

# Plot the density of the slope of the line
plot(density(beta1))
@

Plot of regression
<<>>=
plot(x,y, pch=20)
abline(mean(beta0),mean(beta1))
xx <- seq(min(x), max(x), by=0.01)

ans <- beta0+outer(beta1,xx,'*')
lines(xx,apply(ans,2,quantile,.025), col='blue')
lines(xx,apply(ans,2,quantile,.975), col='blue')

# generate a new data set with the first set of
points(xx, beta0[1]+beta1[1]*xx+rnorm(length(xx),0,sqrt(sigma2[1])), pch=20)

@

Homework Plot the prediction interval from the problem in class

generate a normal using  ans each ans and sigma
% section linear_regression (end)
\renewcommand{\dateTaken}{January 29, 2013}
\daysep
\section{Multiple Linear Regression} % (fold)
\label{sec:multiple_linear_regression}
\lstinputlisting[language=SAS]{code/MultipleRegression.sas}
Raftery-lewis diagnostics should be less than 5.

Let the variance increase proportionaly with the $x$s

<<>>=
vo2 <- read.table("data/03vo2.dat", skip=1, col.names=c("ID","Gender","Age1","BMI","MPH","HR","RPE","MaxVO2ML"))
plot(vo2$MPH, vo2$MaxVO2ML)
plot(vo2$BMI, vo2$MaxVO2ML)
plot(vo2$Gender, vo2$MaxVO2ML)
@


DIC Deviance information criteria (calculated in formula~\ref{eq:deviance})
\begin{align} \label{eq:deviance}
    \text{deviance} = -2\log(\mathcal{L})
\end{align}

\renewcommand{\dateTaken}{January 31, 2013}
\daysep
<<>>=
# Using the VO2 Data
N <- nrow(vo2)
mdl <- '
model {
    for( i in 1:N){
        y[i] ~ dnorm(mu[i], prec)
        mu[i] <- b_0 +
                 b_gen*gender[i] +
                 b_bmi*bmi[i] +
                 b_mph*mph[i] +
                 b_hr*hr[i] +
                 b_rpe*rpe[i] +
                 # b_gen_bmi*gen_bmi[i] +
                 # b_gen_mph*gen_mph[i] +
                 # b_gen_hr*gen_hr[i] +
                 # b_gen_rpe*gen_rpe[i] +
                 # b_bmi_mph*bmi_mph[i] +
                 # b_bmi_hr*bmi_hr[i] +
                 # b_bmi_rpe*bmi_rpe[i] +
                 b_mph_hr*mph_hr[i] # +
                 # b_mph_rpe*mph_rpe[i] +
                 # b_hr_rpe*hr_rpe[i]
    }

    b_0   ~ dnorm(0, .00001);
    b_gen ~ dnorm(0, .001);
    b_bmi ~ dnorm(0, .001);
    b_mph ~ dnorm(0, .001);
    b_hr  ~ dnorm(0, .001);
    b_rpe ~ dnorm(0, .001);

    # b_gen_bmi ~ dnorm(0, .001);
    # b_gen_mph ~ dnorm(0, .001);
    # b_gen_hr  ~ dnorm(0, .001);
    # b_gen_rpe ~ dnorm(0, .001);
    # b_bmi_mph ~ dnorm(0, .001);
    # b_bmi_hr  ~ dnorm(0, .001);
    # b_bmi_rpe ~ dnorm(0, .001);
    b_mph_hr  ~ dnorm(0, .001);
    # b_mph_rpe ~ dnorm(0, .001);
    # b_hr_rpe  ~ dnorm(0, .001);

    vr ~ dgamma(2, .05)
    prec <- 1/vr
}
'
writeLines(mdl, "code/multipleRegression.jags")

y <- vo2[,8]
gender <- vo2[,2]
bmi <- vo2[,4]
hr <- vo2[,6]
mph <- vo2[,5]
rpe <- vo2[,7]

# gen_bmi <- gender*bmi
# gen_hr <- gender*hr
# gen_mph <- gender*mph
# gen_rpe <- gender*rpe
# bmi_mph <- bmi*mph
# bmi_hr <- bmi*hr
# bmi_rpe <- bmi*rpe
mph_hr <- mph*hr
# mph_rpe <- mph*rpe
# hr_rpe <- hr*rpe

jags.data <- c(
"N",
"y",
"gender",
"bmi",
"hr",
"mph",
"rpe",
# "gen_bmi",
# "gen_hr",
# "gen_mph",
# "gen_rpe",
# "bmi_mph",
# "bmi_hr" #,
# "bmi_rpe",
"mph_hr" #,
# "mph_rpe",
# "hr_rpe",
)
jags.params <- c(
"b_0",
"b_gen",
"b_bmi",
"b_hr",
"b_mph",
"b_rpe",
# "b_gen_bmi",
# "b_gen_hr",
# "b_gen_mph",
# "b_gen_rpe",
# "b_bmi_mph",
# "b_bmi_hr",
# "b_bmi_rpe",
"b_mph_hr",
# "b_mph_rpe",
"vr"
)

set.seed(1234)
multr.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   model.file="code/multipleRegression.jags",
                   n.iter=10000, n.burnin=5000, n.chains=1, n.thin=1)
@

\begin{tabular}{cc}
Full & 756.4\\
-hr & 764.4\\
-rpe & 759.2\\
+ all two way interactions & 766.7\\
\end{tabular}

\begin{align*}
    IC= -2\log(\mathcal{L}) + \text{penalty}(\text{parameters})\\
    \text{Deviance} = -2 \log(\mathcal{L})\\
    DIC = -2\log(\mathcal{L}) + pD\\
    pD = \frac{\text{VAR}(\text{deviance})}{2}
\end{align*}
% section multiple_linear_regression (end)
\section{ANOVA} % (fold)
\label{sec:anova}
Cell means model of ANOVA


\begin{tabular}{c|c|c|c}
Treatement 1& Treatement 2 & Treatement 3 & Treatement 4\\
\hline
$y_{11}$ & $y_{21}$ & $y_{31}$ & $y_{41}$ \\
$y_{12}$ & $y_{22}$ & $y_{32}$ & $y_{42}$ \\
$y_{13}$ & $y_{23}$ & $y_{33}$ & $y_{43}$ \\
$y_{14}$ & $y_{24}$ & $y_{34}$ & $y_{44}$ \\
\end{tabular}

so given this data we say that
$y_{1}\sim \mathrm{N}(\mu,\sigma^{2})$

<<tidy=FALSE>>=
anova <- read.table("data/04anova.dat" ,col.names=c("tmt","response","tmt1"))
N <- nrow(anova)
mdl <- 'model {
    # Likelihood
    for (i in 1:N){
        response[i] ~ dnorm(mu[tmt[i]], prec)
    }

    # Create priors for each treatment
    for(i in 1:4){
        mu[i] ~ dnorm(15,.0001)
    }
    prec <- 1/vv
    vv ~ dgamma(1.1,.1)
}
'
writeLines(mdl, "code/ANOVAmodel.jags")
response <- anova$response
tmt <- anova$tmt
@
<<>>=
jags.data <- c("N", "response", 'tmt')
jags.params <- c('mu', 'vv')
anova.sim <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file="code/ANOVAmodel.jags",
                  n.iter=12000,
                  n.chains=1,
                  n.thin=1
                  )
sims <- as.mcmc(anova.sim)
plot(sims, auto.layout=FALSE, ask=FALSE)
autocorr.plot(sims)
raftery.diag(sims)
effectiveSize(sims)
@

<<tidy=FALSE>>=
mdl <- 'model {
    # Likelihood
    for (i in 1:N){
        response[i] ~ dnorm(mu[tmt[i]], prec[tmt[i]])
    }

    # Create priors for each treatment
    for(i in 1:4){
        mu[i] ~ dnorm(15,.0001)
        prec[i] <- 1/vv[i]
        vv[i] ~ dgamma(1.1,.1)
    }
}
'
writeLines(mdl, "code/ANOVAmodelNCvariance.jags")
@
<<>>=
si
@

Devience is the -2*log likelihood

\subsection{Compare Means} % (fold)
\label{sub:compare_means}
We don't care about multiple test becasue we are not under the constraint of a null hypothisis
<<>>=
diff12 <- sims[,2] - sims[,3]
mean( dif12 > 0 )
dif13 <- sims[,2] - sims[,4]
dif14 <- sims[,2] - sims[,5]
dif23 <- sims[,3] - sims[,4]
dif24 <- sims[,3] - sims[,5]
dif34 <- sims[,4] - sims[,5]
mean(dif13>0)
mean(dif14>0)
mean(dif23>0)
mean(dif24>0)
mean(dif34>0)
@
% subsection compare_means (end)

Two by three factorial design.
<<>>=
twoway <- read.table("data/05twoway.dat", col.names=c("block","seedType","inoculate","yield"))
@
Using cell means method

Table should be rotated
\begin{tabular}{c|cc|c}
 & a & b & \\
 \hline
dea & $\mu_{11}$ & $\mu_{12}$ & $\mu_{1\cdot}$\\
con & $\mu_{21}$ & $\mu_{22}$ & $\mu_{2\cdot}$\\
liv & $\mu_{31}$ & $\mu_{22}$ & $\mu_{3\cdot}$\\
\hline
    & $\mu_{\cdot 1}$ & $\mu_{\cdot 2}$ & \\
\end{tabular}

Degrees of freedom is 5, one for type, two for inoculate, and two for interaction

<<tidy=FALSE>>=
y <- as.numeric(twoway$seedType)

mdl <- '
model {
    for (i in 1:24) {
        yield[i] ~ dnorm(mu[ncult[i],ninoc[i]], 1/vv)
    }

    for (i in 1:2) {
        for (j in 1:3) {
            mu[i,j] ~ dnorm(0, .000001)
        }
    }

    vv ~ dgamma(1.5, .1)
}
'

writeLines(mdl, 'code/CultModel.jags')

yield <- twoway$yield
ncult <- as.numeric(twoway$seedType)
ninoc <- as.numeric(twoway$inoculate)

jags.data <- c('yield', 'ncult', 'ninoc')
jags.params <- c('mu','vv')

innits <- function(){list('mu'= matrix(0,2,3), 'vv', 15)}

cult1.jags <- jags( data=jags.data,
                    # inits=innits,
                    parameters.to.save=jags.params,
                    model.file='code/CultModel.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
cult1.sim <- as.mcmc(cult1.jags)
@

Effects model and cell means model are two different ways to do analysis of variance. BYU was a pioneer of the cell means model.

\subsection{ANOVA Marginal Means} % (fold)
\label{sub:anova_marginal_means}

marginals are calculated as $\mu_{1\cdot} = \frac{\mu_{11} + \mu_{12} + \mu_{13}}{3}$
its just a mean of the means
<<>>=
mu1dot <- (cult1.sim[,2] + cult1.sim[,4] + cult1.sim[,6])/3
mu2dot <- (cult1.sim[,3] + cult1.sim[,5] + cult1.sim[,7])/3

plot(mu1dot-mu2dot)
mean(mu2dot-mu1dot>0)

mudot1 <- (cult1.sim[,2] + cult1.sim[,3])/2
mudot2 <- (cult1.sim[,4] + cult1.sim[,5])/2
mudot3 <- (cult1.sim[,6] + cult1.sim[,7])/2

mean(mudot1 - mudot2 < 0) # Probability mu_dot2 is bigger than mu_dot1
mean(mudot3 - mudot2 > 0) # Probability mu_dot3 is bigger than mu_dot2
@
% subsection anova_marginal_means (end)
\subsection{ANOVA Interactions} % (fold)
\label{sub:anova_interactions}
interactions are tested in four cell groups the formula is
\begin{align*}
    \mu_{11}-\mu_{21} = \mu_{12}-\mu_{22}\\
    \mu_{11}-\mu_{21} - \mu_{12}+\mu_{22} = 0
\end{align*}


<<>>=
int1 <- cult1.sim[,2] - cult1.sim[,4] - cult1.sim[,3] - cult1.sim[,4]
int2 <- cult1.sim[,4] - cult1.sim[,6] - cult1.sim[,5] + cult1.sim[,7]
@
% subsection anova_interactions (end)

Finally, compare to seed types that with the best inoculate, does it make sense to pay for the better seed or does the $\mu_{11}-\mu_{21} = \mu_{12}-\mu_{22}$ come from the inoculate
<<>>=
se1 <- cult1.sim[,6] - cult1.sim[,7]
mean(se1<0)
@
\subsection{Linear Algebra Solution} % (fold)
\label{sec:linear_algebra_solution}
% \begin{align*}
%     \begin{bmatrix}
%         27.4\\
%         29.7\\
%         34.5\\
%     \end{bmatrix}
%      =
%      \begin{bmatrix}
%      \end{bmatrix}
% \end{align*}

\[
\underline{\hat{\mu}} = (W' W)^{-1} W' \underline{y}
\]
Least Squares Solution
<<>>=
w  <- rbind(diag(1,6),diag(1,6),diag(1,6),diag(1,6))
muhat <- solve(t(w) %*% w) %*% t(w) %*% twoway$yield
@

\begin{align*}
    y=W\mu\\
    y=WI\mu\\
    y= \underbrace{WA^{-1}}_{X} \underbrace{A\mu}_{\beta}
\end{align*}


\begin{align*}
\begin{bmatrix}
    \frac{1}{6} & \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}\\
    \frac{1}{6} & \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}\\
    \frac{1}{2} & -\frac{1}{2}& 0& \frac{1}{2}& -\frac{1}{2}& 0\\
    0&\frac{1}{2} & -\frac{1}{2}& 0&\frac{1}{2}& -\frac{1}{2}\\
    1 & -1 & 0 & -1 & 1 & 0\\
    0 & 1 & -1 & 0 & -1 & 1\\
\end{bmatrix}
\begin{bmatrix}
    \mu_{11}\\
    \mu_{12}\\
    \mu_{13}\\
    \mu_{21}\\
    \mu_{22}\\
    \mu_{23}\\
\end{bmatrix}
=
\begin{bmatrix}
    \mu_{\cdot\cdot}\\
    \mu_{1\cdot} - \mu_{2\cdot}\\
    \mu_{\cdot1} - \mu_{\cdot2}\\
    \mu_{\cdot2} - \mu_{\cdot3}\\
    \mu_{11}  - \mu_{12} - \mu_{21}+ \mu_{22}\\
    \mu_{12}  - \mu_{13} - \mu_{22}+ \mu_{23}\\
\end{bmatrix}
\end{align*}
% subsection linear_algebra_solution (end)
% section anova (end)
\renewcommand{\dateTaken}{February 12, 2013}
\daysep
<<eval=FALSE>>=
dev <- function(x) {
    fp <- length(y) * log(2* pi)
    sp <-
}
@
\renewcommand{\dateTaken}{February 14, 2013}
\daysep
\section{Analysis of Covariance ANCOVA} % (fold)
\label{sec:analysis_of_covariance_ancova}

Join Me 777-129-200

<<>>=
ac <- read.table("data/06ancova.dat", header=TRUE)
@

First thing you do for analysis is plot the data
<<>>=
plot(ac$speed,ac$scrap,pch=ac$lines)
@

Let's try and come up with a model
\begin{align}
y_{i j} = \beta_{0} + \beta_{1}\cdot\text{Line} + \beta_{2}\cdot\text{Speed} + \beta_{3}\cdot \text{Line} \cdot \text{Speed}
\end{align}

Or we could do this
\begin{align*}
    y_{i j} = \beta_{0 i} + \beta_{1 i}\cdot\text{Speed}
\end{align*}

<<tidy=FALSE>>=
mdl <- 'model {
    for(i in 1:27) {
        scrap[i] ~ dnorm(mu[i], prec);
        mu[i] <- b_0[line[i]] + b_1[line[i]]*speed[i];
    }

    for (i in 1:2) {
        b_0[i] ~ dnorm(30, .001);
        b_1[i] ~ dnorm(0, .01);
    }

    vr ~ dgamma(1.5, .0125);
    prec <- 1/vr;
}
'
writeLines(mdl, 'code/ANCOVAModel.jags')
@
prior for beta not covers 30*100

<<>>=
speed <- ac$speed
line <- ac$line
scrap <- ac$scrap
jags.data <- c('speed', 'line', 'scrap')
jags.params <- c('b_0','b_1', 'vr')
ancova.jags <- jags( data=jags.data,
                    # inits=innits,
                    parameters.to.save=jags.params,
                    model.file='code/ANCOVAModel.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
ancova.jags
@

Diagnostics
<<ANCOVA_Diagnostics,out.width='.19\\textwidth'>>=
ancova.sim <- as.mcmc(ancova.jags)
plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
autocorr.plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
raftery.diag(ancova.sim)
@


Test the probability that the $\beta_{1\cdot}$ are different
<<>>=
b11 <- ancova.sim[,3]
b12 <- ancova.sim[,4]
slopdif <- b11 - b12
mean(slopdif < 0)
mean(slopdif > 0)
@

Try a new model without the different $\beta_{1}$ then compare the DIC

<<tidy=FALSE>>=
mdl <- 'model {
    for(i in 1:27) {
        scrap[i] ~ dnorm(mu[i], prec);
        mu[i] <- b_0[line[i]] + b_1*speed[i];
    }

    b_1 ~ dnorm(0, .01);
    for (i in 1:2) {
        b_0[i] ~ dnorm(30, .001);
    }

    vr ~ dgamma(1.5, .0125);
    prec <- 1/vr;
}
'
writeLines(mdl, 'code/ANCOVAModel_1.jags')
@
prior for beta not covers

<<>>=
speed <- ac$speed
line <- ac$line
scrap <- ac$scrap
jags.data <- c('speed', 'line', 'scrap')
jags.params <- c('b_0','b_1', 'vr')
ancova1.jags <- jags( data=jags.data,
                    # inits=innits,
                    parameters.to.save=jags.params,
                    model.file='code/ANCOVAModel_1.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
ancova1.jags
@

Diagnostics
<<ANCOVA1_Diagnostics,out.width='.19\\textwidth'>>=
ancova1.sim <- as.mcmc(ancova1.jags)
# plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
# autocorr.plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
# raftery.diag(ancova.sim)
@

Lets test differences in line by testing difference in intercept
<<ANCOVA1_Line_Diff>>=
b_0_1 <- ancova1.sim[,1]
b_0_2 <- ancova1.sim[,2]
line_diff <- b_0_1 - b_0_2
mean(line_diff < 0)
mean(line_diff > 0)
plot(density(line_diff), col='blue')
@

<<>>=
plot(ac$speed, ac$scrap, pch=ac$lines)
abline(82.2, 1.22, col='blue')
abline(13.7, 1.3, col='red')

b_0_1 <- ancova.sim[,1]
b_0_2 <- ancova.sim[,2]
b_1_1 <- ancova.sim[,3]
b_1_2 <- ancova.sim[,4]

y1_300 <- 300*b_1_1 + b_0_1
y2_300 <- 300*b_1_2 + b_0_2
diff_300 <- y1_300 - y2_300
mean(diff_300 > 0)
@
% section analysis_of_covariance_ancova (end)
\renewcommand{\dateTaken}{February 21, 2013}
\daysep
\begin{tabular}{ccccccc}
   &   &    & B  &    &    & \\
   &   &  1 &  2 &  3 &  4 & \\
   & 1 &  $\mu_{11}$ &  $\mu_{12}$ &  $\mu_{13}$ &  $mu_{14}$ &  $\mu_{1\bullet}$\\ %$\frac{\mu_{11} +  \mu_{12} +  \mu_{13} +  mu_{14}}{4}=\mu_{1\bullet}$\\
 A & 2 &  $\mu_{21}$ &  $\mu_{22}$ &  $\mu_{23}$ &  $mu_{24}$ &  $\mu_{2\bullet}$\\ %$\frac{\mu_{21} +  \mu_{22} +  \mu_{23} +  mu_{24}}{4}=\mu_{2\bullet}$\\
   & 3 &  $\mu_{31}$ &  $\mu_{32}$ &  $\mu_{33}$ &  $mu_{34}$ &  $\mu_{3\bullet}$\\ %$\frac{\mu_{31} +  \mu_{32} +  \mu_{33} +  mu_{34}}{4}=\mu_{3\bullet}$\\
   & 4 &  $\mu_{41}$ &  $\mu_{42}$ &  $\mu_{43}$ &  $mu_{44}$ &  $\mu_{4\bullet}$\\ %$\frac{\mu_{41} +  \mu_{42} +  \mu_{43} +  mu_{44}}{4}=\mu_{4\bullet}$\\
   % &   & $\frac{\mu_{11} +  \mu_{21} +  \mu_{31} +  mu_{41}}{4}=\mu_{\bullet 1}$
   %     & $\frac{\mu_{12} +  \mu_{22} +  \mu_{32} +  mu_{42}}{4}=\mu_{\bullet 2}$
   %     & $\frac{\mu_{13} +  \mu_{23} +  \mu_{33} +  mu_{43}}{4}=\mu_{\bullet 3}$
   %     & $\frac{\mu_{14} +  \mu_{24} +  \mu_{34} +  mu_{44}}{4}=\mu_{\bullet 4}$\\
\end{tabular}

\section{Multiple Sources of Variation} % (fold)
\label{sec:multiple_sources_of_variation}
\begin{align*}
    \underline{y}= X\underline{\beta} + \underline{\varepsilon}\\
    \varepsilon \sim \mathcal{N}(\underline{0}, \sigma^{2 \underline{T}})\\
    \underline{y} = X \underline{\beta} + Z \underline{u} + e\\
    e\sim \mathcal{N}(\underline{0}, R)\\
    \underline{u} \sim \mathcal{N}(\underline{0}, G)\\
\end{align*}

\begin{align*}
    \expv(Y)= \expv(X \beta + Z u + e) = \expv(X \beta) + \expv(Z u) + \expv(e)= X \beta\\
    \var(Y) = \var(X \beta + Z u + e) = \var( Z u + e)
\end{align*}
If we assume that $u$ and $e$ are independent then we can move forward easily

$V(A\underline{z})=A V(z) A'$
$A V(z) A'$ is a $p \times p$ matrix
\begin{align*}
    \var( Z u + e)= V(Z\underline{u}) + \var(\underline{e}) = Z \var(\underline{u}) Z' + V(\underline{e} = Z G Z' + R
\end{align*}
\renewcommand{\dateTaken}{February 26, 2013}
\daysep
Treatments:
\begin{tabular}{ccc}
Block 1 &  & Block 2\\
I ($y_{11}$) & & I ($y_{12}$)\\
II ($y_{21}$) & & II ($y_{22}$)\\
\end{tabular}

Model in matrix form (Equation~\ref{eq:mv_matrix_model})
\begin{align}\label{eq:mv_matrix_model}
    \begin{bmatrix}
        y_{11}\\
        y_{12}\\
        y_{21}\\
        y_{22}\\
    \end{bmatrix}
    =\begin{bmatrix}
        1 & 0\\
        1 & 0\\
        0 & 1\\
        0 & 1\\
    \end{bmatrix}
\begin{pmatrix}
    \mu_{1}\\
    \mu_{2}\\
\end{pmatrix} +
\begin{bmatrix}
    1 & 0\\
    0 & 1\\
    1 & 0\\
    0 & 1\\
\end{bmatrix}
\begin{pmatrix}
    u_{1}\\
    u_{2}\\
\end{pmatrix} +
\begin{bmatrix}
    e_{11}\\
    e_{12}\\
    e_{21}\\
    e_{22}\\
\end{bmatrix}
\end{align}

\begin{align*}
    e_{ij} \sim^{iid} N(0, \sigma^{2}_{e})\\
    u_{j} \sim^{iid} N(0,\sigma^{2}_{b}) \\
\end{align*}
\begin{align*}
    R &= \begin{bmatrix}
        \sigma^{2}_{e} & 0 & 0 & 0\\
        0 & \sigma^{2}_{e} & 0 & 0\\
        0 & 0 & \sigma^{2}_{e} & 0\\
        0 & 0 & 0 & \sigma^{2}_{e}\\
    \end{bmatrix}\\
    G &= \begin{pmatrix}
        \sigma^{2}_{b} & 0\\
        0 & \sigma^{2}_{b}
    \end{pmatrix}
\end{align*}


\begin{align*}
\begin{bmatrix}
        1 & 0\\
        1 & 0\\
        0 & 1\\
        0 & 1\\
\end{bmatrix}
\begin{pmatrix}
        \sigma^{2}_{b} & 0\\
        0 & \sigma^{2}_{b}
    \end{pmatrix}
    \begin{bmatrix}
        1 & 0 & 1 & 0\\
        0 & 1 & 0 & 1\\
    \end{bmatrix}
    +\begin{bmatrix}
        \sigma^{2}_{e} & 0 & 0 & 0\\
        0 & \sigma^{2}_{e} & 0 & 0\\
        0 & 0 & \sigma^{2}_{e} & 0\\
        0 & 0 & 0 & \sigma^{2}_{e}\\
    \end{bmatrix}\\
    Z G Z' + R = \begin{bmatrix}
        \sigma^{2}_{e} + \sigma^{2}_{b} & 0 & \sigma^{2}_{b} & 0\\
        0 & \sigma^{2}_{e} + \sigma^{2}_{b}  & 0 & \sigma^{2}_{b}\\
        \sigma^{2}_{b}  & 0 & \sigma^{2}_{e} + \sigma^{2}_{b} & 0 \\
        0 & \sigma^{2}_{b} & 0 & \sigma^{2}_{e} + \sigma^{2}_{b}\\
    \end{bmatrix}
\end{align*}
\begin{align*}
    \rho_{y_{11} y_{21}} = \frac{\sigma^{2}_{b}}{\sqrt{\sigma^{2}_{3} + \sigma^{2}_{b}}\sqrt{\sigma^{2}_{3} + \sigma^{2}_{b}}} =\frac{\sigma^{2}_{b}}{\sigma^{2}_{3} + \sigma^{2}_{b}}
\end{align*}
% section multiple_sources_of_variation (end)
<<>>=
twoway <- read.table("data/05twoway.dat", col.names=c("block","seedType","inoculate","yield"))
@

First model with multiple souces of variance, s2blk is a hyper-prior, or a prior on a prior
<<MixedModelModel,tidy=FALSE>>=
mdl <- ' model {
    for (i in 1:21) {
        yield[i] ~ dnorm(mu[i], 1/s2e)
        mu[i] <- aaron[cultn[i], inocn[i]] + u[block[i]]
    }

    for (i in 1:2){
        for (j in 1:3) {
            aaron[i,j] ~ dnorm(30, .001);

        }
    }

    for (i in 1:4) {
        u[i] ~ dnorm(0, 1/s2blk)
    }

    s2e ~ dgamma(1.5, .1);
    s2blk ~ dgamma(1.5, .1);
}
'
writeLines(mdl, 'code/MixedModels.jags')
@

<<MixedModelSimulation>>=
yield <- twoway[, 4]
cultn <- as.numeric(twoway$seedType)
inocn <- as.numeric(twoway$inoculate)
block <- twoway[, 1]

jags.data <- c('yield', 'cultn', 'inocn', 'block')
jags.params <- c('aaron', 's2e', 's2blk')
set.seed(343)
mixedm.jags <- jags(data=jags.data,
                    parameters.to.save=jags.params,
                    model.file='code/MixedModels.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
@

Difference between 1 and 2
<<>>=
mixedm.sim <- as.mcmc(mixedm.jags)
cultivarA <- (mixedm.sim[, 1] + mixedm.sim[, 3] + mixedm.sim[, 5])/3
cultivarB <- (mixedm.sim[, 2] + mixedm.sim[, 4] + mixedm.sim[, 6])/3

mean(cultivarA)
mean(cultivarB)
plot(density(cultivarA - cultivarB))
@
\renewcommand{\dateTaken}{February 28, 2013}
\daysep
Simpler Mixed Model
<<>>=
bond <- read.table("data/07mixedmods.dat", header=TRUE)
metn <- rep(1:3, 21/3)
bond <- cbind(bond, metn)
@
<<>>=
mdl <- ' model{
    for (i in 1:21){
        pressure[i] ~ dnorm(mu[i], 1/s2error);
        mu[i] <- gamma[metn[i]] + u[ingot[i]]
    }

    for (i in 1:3) {
        gamma[i] ~ dnorm(75, 0.001)
    }

    for (i in 1:7) {
        u[i] ~ dnorm(0, 1/s2ing)
    }

    s2error ~ dgamma(1.5, 0.1);
    s2ing   ~ dgamma(1.5, 0.1);
}
'
writeLines(mdl, 'code/SimpleMixedModel.jags')
@
<<>>=
metn     <- bond[, 4]
ingot    <- bond[, 1]
pressure <- bond[, 3]

jags.data   <- c('metn', 'ingot', 'pressure')
jags.params <- c('gamma', 's2error', 's2ing')

bond.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/SimpleMixedModel.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
bond.sim <- as.mcmc(bond.jags)
@

ICC or interclass corelation is a measure of reliability of messurements
\begin{align}
    ICC = \frac{\sigma^{2}_{i}}{\sigma^{2}_{e}+\sigma^{2}_{i}}
\end{align}
<<>>=
icc <- bond.sim[, 5]/(bond.sim[, 6]+bond.sim[, 5])
plot(density(icc))
@

plot something
<<>>=
indif <- bond.sim[, 3] - bond.sim[, 2]
icdif <- bond.sim[, 3] - bond.sim[, 4]
@

\textbf{Back to Complex}
\begin{tabular}{ccccc}
 &  &  & inoc & \\
 &  & con & dea & liv \\
 cult & a & $\alpha_{1 1}$ & $\alpha_{1 2}$ & $\alpha_{1 3}$\\
 & b & $\alpha_{2 1}$ & $\alpha_{2 2}$ & $\alpha_{2 3}$\\
\end{tabular}
<<>>=
int1 <- mixedm.sim[, 2] - mixedm.sim[, 4] - mixedm.sim[, 3] + mixedm.sim[, 5]
int2 <- mixedm.sim[, 4] - mixedm.sim[, 6] - mixedm.sim[, 5] + mixedm.sim[, 7]
plot(density(int1))
plot(density(int2))
@
Let's say that there's no interaction, because

\renewcommand{\dateTaken}{March  5, 2013}
\daysep

\section{Random Coefficients Model} % (fold)
\label{sec:random_coefficients_model}
Genetically identical seeds we treat as subjects

Sixty observations, but only ten wheat types, so there are really somewhere in the middle

\begin{align*}
    \text{yield}_{i j} = \beta_{0} + \beta_{1}\cdot \text{moisture}_{j} + u1_{i} + u2_{i} \cdot \text{moisture}_{j} + \text{error}_{ij}
\end{align*}
so the deviation $u$ has both a slope and an intercept.

% \begin{align*}
%     y=X \beta + Z u + e
% \end{align*}
\begin{align*}
    y = \begin{bmatrix}
        1 & 10\\
        1 & 17\\
        1 & \vdots\\
        \vdots & \ddots\\
    \end{bmatrix}
    \begin{pmatrix}
        \beta_{0}\\
        \beta_{1}
    \end{pmatrix}
    \begin{bmatrix}
        1 & 10 & 0 & 0 & 0 & 0 & \ldots \\
        1 & 57 & 0 & 0 & 0 & 0 & \ldots \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ldots \\
        1 & 40 & 0 & 0 & 0 & 0 & \ldots \\
        0 & 0 & 1 & 16 & 0 & 0 & \ldots \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ldots \\
        0 & 0 & 0 & 0 & 1 & 39 & \ldots \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
    \end{bmatrix}
\begin{pmatrix}
    u0_{1}\\
    u1_{1}\\
    u0_{2}\\
    u1_{2}\\
    u0_{3}\\
    u1_{3}\\
    u0_{4}\\
    u1_{4}\\
    u0_{5}\\
    u1_{5}\\
    u0_{6}\\
    u1_{6}\\
\end{pmatrix}
\end{align*}
\begin{align*}
    G=\begin{bmatrix}
        \sigma^{2}_{int} & 0                & 0                & 0 & \ldots \\
        0                & \sigma^{2}_{slp} & 0                & 0 & \ldots \\
        0                & 0                & \sigma^{2}_{int} & 0 & \ldots \\
        0                & 0                & 0                & \sigma^{2}_{slp} & \ldots \\
        \vdots & \vdots & \vdots & \vdots & \ddots
    \end{bmatrix}
\end{align*}

<<"RandomCoeffModel", tidy=FALSE>>=
wheat <- read.table("data/10randomcoef.dat", header=TRUE)
yield <- wheat$yield
variety <- wheat$variety
moisture <- wheat$moisture
mdl <- ' model {
    for (i in 1:60) {
        yield[i] ~ dnorm(mu[i], 1/s2err)
        mu[i] <- b0 + b1 * moisture[i] + u0[variety[i]] + u1[variety[i]] * moisture[i]
    }

    b0 ~ dnorm(30, 0.001)
    b1 ~ dnorm(0, 0.1)

    for (i in 1:10) {
        u0[i] ~ dnorm(0, 1/s2int)
        u1[i] ~ dnorm(0, 1/s2slp)
    }

    s2err ~ dgamma(1.1, .5)
    s2int ~ dgamma(1.1, .1)
    s2slp ~ dgamma(1.1, 2)
}
'
writeLines(mdl, 'code/RandomCoeff.jags')
@

<<RandomCoeffSim, dependson=c("RandomCoeffModel"), out.width='.16\\textwidth'>>=
jags.data   <- c('yield', 'variety', 'moisture')
jags.params <- c('b0', 'b1', 'u0','u1', 's2err', 's2int', 's2slp')

rc1.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/RandomCoeff.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
rc1.jags
rc1.sim <- as.mcmc(rc1.jags)
plot(rc1.sim, auto.layout=FALSE, ask=FALSE)
@

Find the variance of the $y$
<<>>=
g <- rbind(cbind(18.645, 0), cbind(0, .005))
r <- diag(.392, 60)
z <- model.matrix(~-1+as.factor(variety) + as.factor(variety):moisture, wheat)
Z <- z[,c(1,11,2,12,3,13,4,14,5,15,6,16,7,17,8,18,9,19,10,20)] # need to rearrange so that the intercepts and slopes are interleaved
G <- kronecker(diag(1,10),g)
V <- Z %*% G %*% t(Z) + r
@
\renewcommand{\dateTaken}{March  7, 2013}
\daysep

To get the slope or intercept for the particular variety, add the fixed effect and the random effect for the variety.
% section random_coefficients_model (end)
\section{Hierarchical Models} % (fold)
\label{sec:hierarchical_models}
$y=\mu_{0}+\mu_{s}\text{moisture} + e$

$\mu_{0}~\mathcal{N}(\mu_{0}, \sigma^{2}_{0})$, $\mu_{s}\sim\mathcal{N}(\mu,\sigma^{2}_{s})$, $e\sim(0,\sigma^{2}_{e}$

<<"HierarchicalModel", tidy=FALSE>>=
wheat <- read.table("data/10randomcoef.dat", header=TRUE)
yield <- wheat$yield
variety <- wheat$variety
moisture <- wheat$moisture
mdl <- ' model {
    for (i in 1:60) {
        yield[i] ~ dnorm(mu[i], 1/s2err)
        mu[i] <- u0[variety[i]] + u1[variety[i]] * moisture[i]
    }

    b0 ~ dnorm(30, 0.001)
    b1 ~ dnorm(0, 0.1)

    for (i in 1:10) {
        u0[i] ~ dnorm(beta0, 1/s2int)
        u1[i] ~ dnorm(beta1, 1/s2slp)
    }

    beta0 ~ dnorm(30, 0.001)
    beta1 ~ dnorm(0, 0.1)

    s2err ~ dgamma(1.1, .5)
    s2int ~ dgamma(1.1, .1)
    s2slp ~ dgamma(1.1, 2)
}
'
writeLines(mdl, 'code/Hierarchical.jags')
@

<<HierarchicalSim, dependson=c("HierarchicalModel"), out.width='.16\\textwidth'>>=
jags.data   <- c('yield', 'variety', 'moisture')
jags.params <- c('beta0', 'beta1', 'u0','u1', 's2err', 's2int', 's2slp')

hier.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/Hierarchical.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
hier.jags
hier.sim <- as.mcmc(hier.jags)
plot(hier.sim, auto.layout=inter_plots, ask=inter_plots)
@

Hierarchical models are virtually identical as random coefficients models, but are more pure Bayesian in thought process and methodology.
\begin{lstlisting}
%let notedir = Z:\Dropbox\Active\STAT451\Notes\data;
filename data "&notedir./data/10randomcoef.dat";
filename outfile "&notedir./output/2013-03-07.html";
ods listing close;

data monkeyRatio;
    infile data;
    input  obs variety yield moisture;
run;

ODS GRAPHICS on / imagename="2013-03-07-Plots";
ods html body=outfile (url=none)
         GPATH="&notedir.\figures\";

proc mcmc data=wheat output=wheat_chains nmc=100000 nbi=20000 thin=10 seed=1234 monitor=(_parms_);
    diag(ess autocorr rl) propcov=quanew;
    parms mu0 30 mu1 0 s2 1;
    parms s2int 30 s2slp .1;
    random int ~ normal(mu0,var=s2int) subject=variety monitor=(int_1 int_2);
    random slp ~ normal(mu1,var=s2slp) subject=variety;
    prior mu0 ~ normal(30, var=100);
    prior mu1 ~ normal(0, var=0.1);
    prior s2 ~ dunif(0,3);
    prior s2int ~ dunif(0,50);
    prior s2slp ~ dunif(0,.2);
    mu = int + slp*moisture;
    model yield ~ normal(mu,var=s2);
run;
ods html close;
\end{lstlisting}
% section hierarchical_models (end)
\renewcommand{\dateTaken}{March 12, 2013}
\daysep

\section{Exam Debriefing} % (fold)
\label{sec:exam_debriefing}
Covergence Diagnostics should have included:
\begin{itemize}
    \item Traceplots
    \item Autocorrelation
    \item Effective sample size
    \item Raftery-lewis
\end{itemize}
Model should have been a model with six means and six variances

\begin{tabular}{cccccc}
 &  &   & Gain & &\\
 &  & 1 & 2    & 3 &\\
 Duration & 1 &$\mu_{1}$ & $\mu_{2}$& $\mu_{3}$ & $\frac{\mu_{1}+\mu_{2}+\mu_{3}}{3}$ \\
 & 2 &$\mu_{4}$ & $\mu_{5}$& $\mu_{6}$&$\frac{\mu_{4}+\mu_{5}+\mu_{6}}{3}$ \\
 & & $\frac{\mu_{1}+\mu{4}}{2}$ & $\frac{\mu_{2}+\mu{5}}{2}$ & $\frac{\mu_{3}+\mu{6}}{2}$ &
\end{tabular}

Next exam will probably have
\begin{itemize}
    \item a random effects and or hierarchal model
    \item Non-normal likelihood
\end{itemize}
% section exam_debriefing (end)
\section{Logistic Regression} % (fold)
\label{sec:logistic_regression}
What we have done so far:
\begin{itemize}
    \item ANOVA
    \item Regression
    \item ANCOVA --- Both Categorical and Continuous predictors
    \item Mixed --- More than one source of variability
\end{itemize}

\begin{tabular}{c c| c c}
 &  & \textbf{Response} & \\
 &  & Categorical & Continuous \\
 \hline
 \textbf{Predictors} & Categorical & & ANOVA \\
 & Continuous & & Regression
\end{tabular}

$y=\binomd(n,\pi)$, with $0 \le \pi \le 1$

Odds: $\frac{\pi}{1-\pi}$

Sports ``3 to 1'' is 3 failures for every one success or $\pi=.25$

$Odds > 0$, 1 should be the midpoint

odds have a skewed distribution

\begin{tabular}{c|c|c}
 \hline
 probability & Odds & $\log(\text{Odds)}$\\
 $\pi$ & Success-Failure & $\log(S/F)$\\
 \hline
 $\frac{1}{5}$ & 1 - 1 & 0\\
 $\frac{1}{5}$ & 1 - 4 & -1.39\\
 $\frac{4}{5}$ & 4 - 1 & 1.39\\
 \hline
\end{tabular}

<<CHDData>>=
chd <- rbind(
c(17, 274, 0, 0, 0),
c(15, 122, 0, 1, 0),
c(7, 59, 0 ,0, 1),
c(5,32,0,1,1),
c(1,8,1,9,9),
c(9,39,1,1,0),
c(3,17,1,0,1),
c(14,58,1,1,1)
)
colnames(chd) <- c('CHD','nRisk','Cat','agegrp','abECG')
tmt <- seq(8)
chd <- cbind(chd,tmt)
CHD <- chd[,1]
nRisk <- chd[,2]
tmt <- chd[,6]
@

Cell means model
<<LogitCellMeansModel,tidy=FALSE>>=
mdl <- ' model {
    for (i in 1:8) {
        CHD[i] ~ dbin(p[i], nRisk[i]);
            logit(p[i]) <- b[tmt[i]];
            b[i] ~ dnorm(0, .1);
    }
}
'
writeLines(mdl,'code/LogitBinomial.jags')
@
<<LogitCellMeansSim, dependson=c('LogitCellMeansModel','CHDData')>>=
jags.data   <- c('CHD', 'nRisk', 'tmt')
jags.params <- c('b')

# innits <- list(list('p' <- runif(8,0,1)))

logit.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/LogitBinomial.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
@
\renewcommand{\dateTaken}{March 14, 2013}
\daysep
\begin{align*}
    \text{logit}(p_{i})&=b_{i}\\
    \log(\frac{p_{i}}{1-p_{i}})&=b_{i}\\
    \frac{p_{i}}{1-p_{i}}&=e^{b_{i}}\\
    p_{i} &= e^{b_{i}}-pe^{b_{i}}\\
    p_{i} + p_{i}e^{b_{i}}&=e^{b_{i}}\\
    p_{i} &= \frac{e^{b_{i}}}{1+e^{b_{i}}} \frac{e^{-b_{i}}}{e^{-b_{i}}}\\
    p_{i} &= \frac{1}{e^{-b_{i}+1}}\\
\end{align*}
<<>>=
logit.sim <- as.mcmc(logit.jags)
p1 <- 1/(exp(-logit.sim [,1])+1)
plot(density(p1))
@
<<>>=
jags.params <- c('b','p')
logit.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/LogitBinomial.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
logit.sim <- as.mcmc(logit.jags)
@
factorial design

Cat-N
\begin{tabular}{cccc}
      &  & abEUG & \\
      &  & Y & O\\
abECG & N & $\mu_1$ & $\mu_2$ \\
      & Y & $\mu_3$ & $\mu_4$ \\
\end{tabular}

Cat-Y
\begin{tabular}{cccc}
      &  & abEUG & \\
      &  & Y & O\\
abECG & N & $\mu_5$ & $\mu_6$ \\
      & Y & $\mu_7$ & $\mu_8$ \\
\end{tabular}

To find the marginals for cat
\begin{align}
    \text{Cat-n} = \frac{\mu_1+\mu_2+\mu_3+\mu_4}{4}
    \text{Cat-y} = \frac{\mu_1+\mu_2+\mu_3+\mu_4}{4}
\end{align}
% Dr Reese came to class today
% \begin{itemize}
%     \item B is a log of an odds
%     \item B is a ratio
% \end{itemize}
<<>>=
p1 <- logit.sim[,10]
p2 <- logit.sim[,11]
p3 <- logit.sim[,12]
p4 <- logit.sim[,13]
p5 <- logit.sim[,14]
p6 <- logit.sim[,15]
p7 <- logit.sim[,16]
p8 <- logit.sim[,17]
@
Find the marginals
<<>>=
catno <- (p1+p2+p3+p4)/4
catyes <- (p5+p6+p7+p8)/4
plot(density(catno), xlim=c(.05,.4), lwd=2)
lines(density(catyes), lwd=2, col='red')

young <- (p1+p3+p5+p7)/4
old <- (p2+p4+p6+p8)/4
plot(density(young), xlim=c(.05,.4), ylim=c(0,15), lwd=2)
lines(density(old), lwd=2, col='red')

abNo <- (p1+p2+p5+p6)/4
abYes <- (p3+p4+p7+p8)/4
@

To compare old to young we use an odds ratio or a risk ratio
<<out.width='\\textwidth'>>=
ORage <- (old/(1-old))/(young/(1-young))
mean(ORage>1)

RRage <- old/young
mean(RRage>1)

ORab <- (abYes/(1-abYes))/(abNo/(1-abNo))
ORCat <- (catyes/(1-catyes))/(catno/(1-catno))

mean(ORab)
mean(ORCat)
par(mfrow=c(2,2))
xlims <- c(0,0.5)
ylims <- c(0,20)
plot(density(catno), xlim=xlims, ylim=ylims)
lines(density(catyes), col='blue')
plot(density(abNo), xlim=xlims, ylim=ylims)
lines(density(abYes), col='blue')
plot(density(young), xlim=xlims, ylim=ylims)
lines(density(old), col='blue')
par(mfrow=c(1,1))
@

two way interaction for abnormal ECG and age averaged over catacolamines
\begin{align*}
    \mu_{1} + \mu_{5} - \mu_{2} - \mu_{6} - \mu_{3} - \mu_{7} + \mu_{4} + \mu_{8} = 0
\end{align*}

to average across abnormal ECG, rewrite the tables
abECG-N
\begin{tabular}{cccc}
      &  & abEUG & \\
      &  & Y & O\\
Cat   & N & $\mu_1$ & $\mu_2$ \\
      & Y & $\mu_5$ & $\mu_6$ \\
\end{tabular}

abECG-Y
\begin{tabular}{cccc}
      &  & abEUG & \\
      &  & Y & O\\
Cat   & N & $\mu_3$ & $\mu_4$ \\
      & Y & $\mu_7$ & $\mu_8$ \\
\end{tabular}
\begin{align*}
    \mu_{1} + \mu_{3} - \mu_{2} - \mu_{4} - \mu_{5} - \mu_{7} + \mu_{6} + \mu_{8} = 0
\end{align*}
<<>>=
intageab <- p1+p5-p2-p6-p3-p7+p4+p8
plot(density(intageab))
@
\renewcommand{\dateTaken}{March 19, 2013}
\daysep
\subsection{Logistic Regression In SAS} % (fold)
\label{sub:logistic_regression_in_sas}

JAGS freefloating, SAS contstrained

\begin{tabular}{cccc}
\hline
Cat-N &  & Age & \\
      &  & Y & O\\
ECG   & N & $p_1$, .07, .06 & $p_2$, .12, .12 \\
      & Y & $p_3$, .10, .12 & $p_4$, .16, .16 \\
      \hline
\end{tabular}

\begin{tabular}{cccc}
\hline
Cat-Y &  & Age & \\
      &  & Y & O\\
ECG   & N & $p_5$, .12, .15 & $p_6$, .20, .23 \\
      & Y & $p_7$, .17, .18& $p_8$,  .26, .24\\
      \hline
\end{tabular}

SAS:
\begin{align*}
\log\left(\frac{p}{1-p}\right) & =\beta_{0}+\beta_{\text{cat}}+\beta_{\text{age}}+\beta_{\text{ecg}}\\
    p & =\frac{1}{1+e^{-x\beta}}\\
    p_{5} &= \frac{1}{1+e^{-\beta_{0}-\beta_{\text{cat}}}}
\end{align*}
This model has 4 degrees of freedom

\begin{align*}
    X = \begin{bmatrix}
        1 & 0 & 0 & 0\\
        1 & 0 & 1 & 0\\
        1 & 0 & 0 & 1\\
        1 & 0 & 1 & 1\\
        1 & 1 & 0 & 0\\
        1 & 1 & 1 & 0\\
        1 & 1 & 0 & 1\\
        1 & 1 & 1 & 1\\
    \end{bmatrix}
\end{align*}
<<>>=
1/(1+exp(2.6263-.6199))
@

JAGS Model
\begin{align*}
    \log\left(\frac{p_{i}}{1-p_{i}}\right)=b_{i}\qquad i=\{1,\ldots,8\}
\end{align*}
this model has 8 degrees of freedom

<<LogitReducedModel, tidy=FALSE>>=
mdl <- ' model {
    for (i in 1:8) {
        CHD[i] ~ dbin(p[i], nRisk[i]);
            logit(p[i]) <- bint + bcat*cat[i] + bage*age[i] + becg*ecg[i];
    }

    bint ~ dnorm(0, .1);
    bcat ~ dnorm(0, .1);
    bage ~ dnorm(0, .1);
    becg ~ dnorm(0, .1);
}
'
writeLines(mdl,'code/LogitReducedModel.jags')
@
<<LogitReducedSim, dependson=c('LogitReducedModel','CHDData')>>=
CHD <- chd[,1]
nRisk <- chd[,2]
tmt <- chd[,6]
cat <- chd[,3]
age <- chd[,4]
ecg <- chd[,5]

jags.data   <- c('CHD', 'nRisk', 'cat','age','ecg')
jags.params <- c('bint', 'bcat', 'bage','becg','p')

set.seed(12)
logit.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/LogitReducedModel.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
@
SAS's Deviance is calculate as $\text{Dbar}-\text{Dmean}$

computing $pD=\frac{\var(\text{deviance})}{2}$ is a better way to calculate it in general
% subsection logistic_regression_in_sas (end)
\subsection{Random Effects} % (fold)
\label{sub:random_effects}
<<SeedNewData>>=
seeds <- read.table("data/seednew.dat", stringsAsFactors = FALSE, skip=1, col.names=c("r","n","tmt","nseed","ntype"))
sum(seeds$n[seeds$tmt==1])
sum(seeds$r[seeds$tmt==1])

var(seeds$r[seeds$tmt==1])
@
Is the variability reasonable?

<<LogitMixedModel, tidy=FALSE>>=
mdl <- 'model {
    for (i in 1:21) {
        r[i] ~ dbin(p[i], n[i])
        logit(p[i]) <- b[tmt[i]] + e[i]
    }

    for (i in 1:4) {
        b[i] ~ dnorm(0, 1)
    }

    for (i in 1:21) {
        e[i] ~ dnorm(0, prec)
    }
    s2 ~ dunif(0,2);
    prec <- 1/s2
}
'
writeLines(mdl, 'code/LogitMixedModel.jags')
mdl <- 'model {
    for (i in 1:21) {
        r[i] ~ dbin(p[i], n[i])
        logit(p[i]) <- b[tmt[i]]
    }

    for (i in 1:4) {
        b[i] ~ dnorm(0, 1)
    }
}
'
writeLines(mdl, 'code/Logit09Simplified.jags')
@
<<LogitMixedModelSim, dependson=c('LogitMixedModel')>>=
r <- seeds[,1]
n <- seeds[,2]
tmt <- seeds[,3]
jags.data <- c('r','n','tmt')
jags.params <- c('b','e')

seed.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/LogitMixedModel.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)

jags.params <- c('b')
seedsimple.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/Logit09Simplified.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
seed.jags
seedsimple.jags
@
% subsection random_effects (end)
\renewcommand{\dateTaken}{March 21, 2013}
\daysep
\lstinputlisting[language=SAS]{code/2013-03-21_SAS.sas}
% section logistic_regression (end)
\section{Poisson Likelihood} % (fold)
\label{sec:poisson_likelihood}
11epi.dat Epilepsy
\begin{itemize}
    \item ``pid'' --- Patient ID
    \item ``scount'' --- Seizure Count
    \item ``tmt'' --- Treatment
    \item ``base'' --- Base Effect
    \item ``age'' --- Age
    \item ``visit''
\end{itemize}

$S$ is the number of seizures (scount)
<<EPIData>>=
epi <- read.table('data/11epi.dat', col.names=c("pid", "scount", "tmt", "base", "age", "visit"))
N <- nrow(epi)
pid <- epi$pid
N_pid <- length(unique(pid))
scount <- epi$scount
tmt <- epi$tmt
base <- epi$base
age <- epi$age
visit <- epi$visit
epi$indic4 <- ifelse(visit==4, 1, 0)
indic4 <- epi$indic4
@
\begin{align*}
    S_{i} &\sim \poisd(\lambda)\\
    \lambda = a_{0} + a_{\text{age}}*\text{age}_{i} + a_{\text{tmt}}*\text{tmt}_{i} + a_{\text{base}}*\text{base}_{i}
\end{align*}

\renewcommand{\dateTaken}{March 26, 2013}
\daysep
Plotting data
<<>>=
library(nlme)
g_epi <- groupedData(scount ~ visit | pid, epi)
plot(g_epi)
plot()
@

Things learned from this plot:
\begin{itemize}
    \item There doesn't seem to be an effect due to time
    \item Subjects matter, so we need a random
\end{itemize}

<<>>=
curve(dgamma(x,1.1,0.1), from=0, to=20)
@
<<EPIModel>>=
mdl <- 'model {
    for (i in 1:N) {
        scount[i] ~ dpois(lambda[i])
        lambda[i] <- a_0 + a_age*age[i] + a_base*base[i] + a_tmt*tmt[i] + int_age_base*age[i]*base[i] + int_age_tmt*age[i]*tmt[i] + int_base_tmt*base[i]*tmt[i] + u_sub[pid[i]]
    }

    # priors
    a_0        ~ dgamma(1.1, 0.1)
    a_age      ~ dgamma(1.1, 0.1)
    a_base     ~ dgamma(1.1, 0.1)
    a_tmt      ~ dgamma(1.1, 0.1)
    int_age_base ~ dgamma(1.1, 0.1)
    int_age_tmt  ~ dgamma(1.1, 0.1)
    int_base_tmt ~ dgamma(1.1, 0.1)

    for (i in 1:59) {
        u_sub[i] ~ dexp(ll)
    }
    ll ~ dunif(0,1)
}
'
writeLines(mdl, 'code/EPI_Interaction.jags')
@
<<EPISim,dependson=c('EPIData','EPIModel')>>=
jags.data <- c('N', 'pid', 'scount', 'age', 'tmt', 'base')
jags.params <- c('a_0', 'a_age', 'a_base', 'a_tmt', 'int_age_base', 'int_age_tmt', 'int_base_tmt', 'll')
set.seed(3245)
epi.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/EPI_Interaction.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
epi.jags
@
<<EPIDiagnostics>>=
epi.sim <- as.mcmc(epi.jags)
plot(epi.sim, auto.layout=inter, ask=inter)
autocorr.plot(epi.sim, auto.layout=inter, ask=inter)
raftery.diag(epi.sim)
@
Posterior Predictive Plots
<<>>=
hist(epi$scount)
@
We can't do general poserior predicitves so let's look at particular values
<<>>=
hist(epi$scount[epi$tmt==1 & epi$age>=19 & epi$age<=23 & 30<=epi$base & epi$base<=60 ])
plot(density(epi$age))
curve(dnorm(x,30,8),add=TRUE, col='red')
plot(density(epi$base))
curve(dgamma(1.1, scale=30), add=T, color='red')
people <- cbind(age=rnorm(iter,30,8), )

iter<- 10^4
pp <- numeric(iter)
for(i in 1:iter){
    rage <- rnorm(1,30,8)
    rbase <- rgamma(1,1.1, scale=30)
    rtmt <- rbinom(1,1,.5)
    x1 <- epi.sim[i,1] + epi.sim[i,2]*rage + epi.sim[,3]*rbase + epi.sim[,4]*rtmt + epi.sim[i,6]*rage*rbase + epi.sim[i,7]*rage*rtmt + epi.sim[i,8]*rbase*rtmt + rexp(1,epi.sim[i,9])
    pp[i] <- rpois(1,x1)
}
hist(epi$scount, breaks=20, freq=F, col='red')
hist(pp,breaks=20, freq=F, add=T)
@
\subsection{Log Poisson} % (fold)
\label{sub:log_poisson}
\renewcommand{\dateTaken}{March 28, 2013}
\daysep
\begin{lstlisting}
count[i] <- dpois[lambda[i]]
log(lambda[i]) <- a0 + a_age*log(age[i]) + a_base*log(base[i]/4) + a_tmt*tmt[i] +
                  a_base_tmt*log(base[i]/4)+tmt[i] + a_4*indic4[i] +
                  u[pid[i]] + aobs[i]
aobs[i] ~ dnorm(0,1/s2obs)
\end{lstlisting}
<<EPIPaperModel, tidy=FALSE, dependson=c('EPIDATA')>>=
lbase4 <- log(base/4)
lage <- log(age)
mdl <-'model {
    for (i in 1:N) {
        scount[i] ~ dpois(lambda[i])
        log(lambda[i]) <- a_0 + a_age*lage[i] + a_base*lbase4[i] + a_tmt*tmt[i] +
                          a_base_tmt*lbase4[i]*tmt[i] + a_4*indic4[i] +
                          u_sub[pid[i]] + u_obs[i]
        u_obs[i] ~ dnorm(0, 1/s2obs)
    }
    for (i in 1:N_pid) {
        u_sub[i] ~ dnorm(0, 1/s2sub)
    }

    a_0        ~ dnorm(0, 0.0001)
    a_base     ~ dnorm(0, 0.0001)
    a_tmt      ~ dnorm(0, 0.0001)
    a_base_tmt ~ dnorm(0, 0.0001)
    a_age      ~ dnorm(0, 0.0001)
    a_4        ~ dnorm(0, 0.0001)
    s2obs      ~ dgamma(1.1, 1)
    s2sub      ~ dgamma(1.1, 1)
}
'
writeLines(mdl, 'code/EPIPaper.jags')
@
<<EPIPaperSim, dependson=c('EPIPaperModel')>>=
jags.data <- c('N', 'N_pid', 'pid', 'scount', 'lage', 'tmt', 'lbase4', 'indic4')
jags.params <- c('a_0', 'a_age', 'a_base', 'a_tmt', 'a_base_tmt', 'a_4', 's2obs', 's2sub')
a <- 'test'
set.seed(3245)
epiP.jags <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file='code/EPIPaper.jags',
                  n.iter=12000, n.burnin=2000,
                  n.chains=1, n.thin=1)
@
If you have correlated $X$'s then you shift the $X$
% subsection log_poisson (end)
% section poisson_likelihood (end)
\end{document}
