\documentclass[12pt,letterpaper,oneside]{article}
\newcommand{\university}{Brigham Young University}
\newcommand{\classCode}{STAT 451}
\newcommand{\className}{Applied Bayesian Statistics}
\newcommand{\documentName}{Notes}
\newcommand{\dateTaken}{January 7, 2013}
\newcommand{\semester}{Winter 2013}
\newcommand{\teacher}{Dr. Gil Fellingham}
\newcommand{\docVersion}{00000}
\newcommand{\authorName}{\href{mailto:adam@acjackman.com}{Adam Jackman}}

\input{./Styles/code.tex}
\input{./Styles/pageStyle.tex}

\input{./Styles/mathCommands.tex}

<<KnitrSettings, cache=FALSE, include=FALSE>>=
# Adam Jackman
# December 3, 2012
opts_chunk$set(fig.path='figure/graphic-', cache.path='cache/graphic-', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', out.width='.4\\textwidth', cache=TRUE, par=TRUE, size="scriptsize")
options(width=80)
knit_hooks$set(crop = hook_pdfcrop)
# render_listings() # Use listings instead of regular output

library(R2jags)
@

\begin{document}
\input{./Styles/title.tex}
\renewcommand{\dateTaken}{January 8, 2013}
\daysep

\section{Class Introduction} % (fold)
\label{sec:class_introduction}
Office Hours Monday

Keep a copy of the code

Preliminary assignment email now.
% section class_introduction (end)

\section{Bayesian Methodology} % (fold)
\label{sec:bayesian_methodology}
Parameterize state of knowledge

Estimate parameters

\begin{align*}
    f(\underline{\theta}|\underline{x})=\frac{L(\underline{x}|\underline{\theta})\prod(\underline{\theta})}{\iint_{\underline{\theta}} L(\underline{x}|\underline{\theta})\prod(\underline{\theta}) \dd \underline{\theta}} \propto L(\underline{x}| \underline{\theta})\prod(\underline{\theta})
\end{align*}
\subsection{Metropolis Sampler} % (fold)
\label{sub:metropolis_sampler}

\begin{align*}
    \pi\left[\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1} \exp\left( -\frac{x}{\beta}\right)\right]+ (\pi-1)\left[\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1} \exp\left( -\frac{x}{\beta}\right)\right]\\
    \frac{1}{4}\left[\frac{1}{\Gamma(2)3^{2}}x^{2-1} \exp\left( -\frac{x}{3}\right)\right] +
    \frac{3}{4}\left[\frac{1}{\Gamma(4)3^{4}}x^{4-1} \exp\left( -\frac{x}{3}\right)\right]\\
    k=4\Gamma(4) 3^3
\end{align*}
\begin{align*}
    g(x)=k \cdot f(x)= 18 x \exp\left(-\frac{x}{3}\right)+ x^3\exp\left(-\frac{x}{3}\right)
\end{align*}
\subsubsection{Metropolis Sampler Steps} % (fold)
\label{ssub:metropolis_sampler_steps}
\begin{itemize}
    \item Initial $x=x_0$
    \item for $i=1$ to $n$
    \begin{itemize}
        \item generate candidate $c$
        \item $\displaystyle{\frac{g(c)}{g(x_{i-1}} = r}$
        \item let $x_i=c$ with $\prob=\min(1,r)$
    \end{itemize}
\end{itemize}
% subsubsection metropolis_sampler_steps (end)
\subsubsection{Metropolis Code} % (fold)
\label{ssub:metropolis_code}
<<metropolisSampler,cache=TRUE>>=
# Define g function
g <- function(x){
18 * x * exp(-x/3) + x^3 * exp(-x/3)
}

# Write Sampler
out <- NULL
out[1] <- 5
candidate.sigma <- 20
counter <- 0
samples <- 10^4
for (i in 2:samples) {
    out[i] <- out[i - 1]
    candidate <- rnorm(1, out[i-1], candidate.sigma)
    if (candidate > 0){
        r <- g(candidate)/g(out[i-1])
        if (r > runif(1,0,1)){
            out[i] <- candidate
            counter <- counter +1
        }
    }
}
message(counter/samples) # should be about 24-40
plot(out, type='l', main="Trace Plot")
@
Trace Plot looks good

<<MetropolisSamplerPosteriorPlot>>=
xx <- seq(0,50,length=100000)
k <- 4*gamma(4)*3^3
# Plot the exact distribution
plot(xx,g(xx)/k, lwd=2, col='blue')
# curve(.25*dgamma(x, shape=2, scale=3)+.75*dgamma(x, shape=4, scale=3), add=TRUE, col='blue')
# Both of these should produce the same line

lines(density(out),lwd=2)

# Exact Mean
.25*(2*3)+.75*(4*3)

# Approximate Mean
mean(out)
@
% subsubsection metropolis_code (end)
% subsection metropolis_sampler (end)
% section bayesian_methodology (end)
\renewcommand{\dateTaken}{January 10, 2013}
\daysep

Paul Sabin --- Stat 451 TA\\
Tuesday 2:30-4:00\\
Friday: 9:30-11:00\\
Office: 233\\
Email:\href{mailto:r.paul.sabin@gmail.com}{r.paul.sabin@gmail.com}

<<eval=FALSE>>=
# For Winbugs
install.packages("ARM", "R2WinBUGS")

# For jags
install.packages("R2jags")
@

\section{Two Sample $t$ test} % (fold)
\label{sec:two_sample_t_test}

\begin{align*}
y \sim \mathcal{N}(\mu, \sigma^2)\quad i=1,2\\
f(\mu_1)=\mathcal{N}(\mu=0, \sigma^2=1000)
f(\sigma^2)=\text{Unif}(0, 5000)
\end{align*}
% section two_sample_t_test (end)

PROC MCMC two group $t$ test
\begin{lstlisting}[language=SAS]
data examp2;
infile dataset;
input tmt response;
run;

/*
nmc: Number of Marcov Chains
nbi: Number Burn in

dic: information criteria
*/
proc mcmc data=examp2 outpost=post2 seed=1234 nmc=100000 nbi=5000
                      statistics(alpha=.05)=(summary interval) thin=10
                      moniter=( parms mudif varratio) propcov=quannew
                      diagnostics=(all) dic ;
array mu[2] mu1-mu2;
arra sig2[2] sig21-sig22;
prams: mu: sig2:;
prior mu: ~ normal(0, sd=10000);
prior sig21 ~ unif(9, 5000); /*  gamma(shape=30, scale=50) */
prior sig22 ~ unif(9, 5000);
mudif = mu1 - mu2;
varratio = sig21/sig22;
mm = mu[tmt];
vv = sig2[tmt];
model response ~ normal(mm, var=vv);
run;

proc export data=post2 outfile='twogroups.csv' dbms=csv replace;
run;
\end{lstlisting}

\begin{description}
    \item[HPD --- Highest Posterior Density] the smallest interval
\end{description}

Fischer variance problem --- a two sample $t$ test with unequal variances


% <<engine='SAS'>>=
% data foo;
% input a b c;
% datalines;
% 1 2 3
% 4 5 6
% 7 8 9
% ;
% run;
% @

Check the posterior autocorolations

\renewcommand{\dateTaken}{January 15, 2013}
\daysep
<<eval=FALSE>>=
    library(rjags)
    library(R2jags)
@
JAGS uses precisions, not variances. Precisions are $\frac{1}{\sigma^2}$

\lstinputlisting[language=R]{code/TwoSampleJAGS.R}


\renewcommand{\dateTaken}{January 17, 2013}
\daysep

You need to think about priors

\section{Diagnostics} % (fold)
\label{sec:diagnostics}
<<>>=
library(R2jags)
mdl <- "
    model {
        for (i in 1:33) {
            y[i] ~ dnorm(mu[tmt[i]], prec[tmt[i]])
        }

        for (i in 1:2) {
            mu[i] ~ dnorm(0,0.000001)
            vr[i] ~ dunif(0,5000)
            prec[i] <- 1/vr[i]
        }
    }
"
groups <- read.table("data/01twogroups.dat", col.names=c("tmt", "y"))
writeLines(mdl, "code/twogroups.jags")
tmt <- groups$tmt
y <- groups$y
# Data to go into jags
data.jags <- c('tmt','y')
# what parameters to keep track of
parms <- c('mu','vr')
# Initial Values
innts <- function() {list('mu' = rnorm(2,125,5), 'vr' = runif(2,0,5000))}
twogroups.sim <- jags(data=data.jags, parameters.to.save=parms, inits=innts, model.file="code/twogroups.jags",
    n.iter=6000, n.burnin=1000, n.chains=1, n.thin=1)

twogroups.sim

sims <- as.mcmc(twogroups.sim)
plot(sims, auto.layout=FALSE, ask=FALSE)

# plot(sims[,2], type='l')
@

First diagnostic: Look at your trace plot

Second diagnostic: look at auto corilation
<<>>=
autocorr(sims)
autocorr.plot(sims)
@

A good autocorrelation plot shows the first bar being tall, but the rest being relatively small.

Fourth diagnostic: Effective Sample size
<<>>=
effectiveSize(sims)
@
these should be relatively close to the actual sample size

fifth diagnostic: Raftery Lewis Diagnostic
<<>>=
raftery.diag(sims)
@
Check if you are within .005 (in the tails) with high probability

Dependence Factor should be less than 5

These five are the most commonly used, but there are others
<<>>=
geweke.diag(sims)
@
Tests for drift in the samples. does something like a $t$ test on the first 10\% and the last 50\% of the chain.

Heild
<<>>=
heidel.diag(sims)
@
test of stationarity, or convergence

both the stationarity and half width mean test should pass.


Gelmen needs chains started at different points
% section diagnostics (end)
\renewcommand{\dateTaken}{January 22, 2013}
\daysep
\section{Posterior Predictives} % (fold)
\label{sec:posterior_sredictives}
Liklihood $g_{i1}\sim \mathcal{N}(\mu_1, \sigma^2_1$, $g_{i2}\sim \mathcal{N}(\mu_2, \sigma^2_2$

Get a new y by plugging the sampled values of the parameters into a random sampler for the likelihood
\begin{lstlisting}
ynew_1 <- rnorm(1,mu1[1],sigma1[1])
\end{lstlisting}

\begin{tabular}{ccccc}
 $\mu_1$ & $\sigma^2_1$ & $\mu_2$ & $\sigma^2_2$ & $y_{\text{new}1}$\\
 \vdots & \vdots & \vdots & \vdots & \vdots
\end{tabular}

<<>>=
ratios <- read.table("../Homework/01/monkeyratio.dat")[,1]
library(R2jags)
N <- length(ratios)

mdl <- "
    model {
        for (i in 1:N) {
            ratios[i] ~ dbeta(alpha, beta)
        }

        alpha ~ dgamma(5, .025)
        beta ~ dgamma(5, .025)
    }
"
writeLines(mdl, "code/hw1.jags")

jags.data <- c("ratios", "N")
jags.params <- c("alpha", "beta")
jags.inits <- function() {
    list('alpha' <- rgamma(5,.025), 'beta' <- rgamma(5,.025))
}
set.seed(5487)

ratios.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   # inits=jags.inits,
                   model.file="code/hw1.jags",
                   n.iter=6000, n.burnin=1000, n.chains=1, n.thin=1)

sims <- as.mcmc(ratios.sim)

# autocorr.plot(sims)
alpha <- sims[,1]
beta <- sims[,2]
ynew <- rbeta(10000,alpha,beta)

# Plot of the density of the mean and the posterior predictive
plot(density(alpha/(alpha+beta)), xlim=c(.6,.9))
lines(density(ynew), col='blue') # Plot of the posterior predictive
@

 We don't make up data we get it from ``Pedagogically flexible data acquisition.''---Dr Tolly.


You can use other priors, they just may not be as easy for example:
<<include=FALSE>>=
mdl <- '
model {
    for (i in 1:5) {
        y[i] ~ dnorm(mu, prec)
    }

    mu ~ dunif(0,1)
    s2 ~ dunif(0,.05)
    prec <- 1/s2
}
'
writeLines(mdl,"code/hw1norm.jags")
y <- ratios
jags.data <- 'y'
jags.params <- c('mu','s2')
hw1norm.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   model.file="code/hw1norm.jags",
                   n.iter=6000, n.burnin=1000, n.chains=1, n.thin=1)
sims <- as.mcmc(hw1norm.sim)
plot(sims, auto.layout=FALSE, ask=FALSE)

@
% section posterior_predictives (end)
\section{Linear Regression} % (fold)
\label{sec:linear_regression}
Likelihood \[ y_{i} ~ \mathcal{N}(\mu_{i}, \sigma^2)\]

The mean is different for every data point, but the variance is constant.

\begin{align*}
    \mu_1 \leftarrow \beta_{0} + \beta_{1} x\\
    \beta_{0} \sim \mathcal{N}(,\tau=.001)\\
    \beta_{1} \sim \mathcal{N}(\mu=0,\tau=.001)\\
    \sigma^{2} \sim \\
\end{align*}
\renewcommand{\dateTaken}{January 24, 2013}
\daysep

<<>>=
jags.modelfile <- 'code/linreg.jags'
mdl <- '
    model {
        for (i in 1:N) {
            y[i] ~ dnorm(mu[i], prec)
            mu[i] <- beta_0 + beta_1 * x[i]
        }

        beta_0 ~ dnorm(0, .00001)
        beta_1 ~ dnorm(0, .001)
        sigma2 ~ dgamma(1.1, .5)
        prec <- 1/sigma2
    }
'
writeLines(mdl, 'code/linreg.jags')


linregdata <- read.table("data/02linreg.dat", col.names=c('lotsize', 'manhours'))
N <- nrow(linregdata)
y <- linregdata[,2]
x <- linregdata[,1]
jags.data <- c('y', 'x', 'N')
jags.params <- c('beta_0', 'beta_1', 'sigma2')


set.seed(5487)
linreg.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   model.file='code/linreg.jags',
                   n.iter=10000, n.burnin=5000, n.chains=1, n.thin=1)
linreg.sim
@

Let's check for convergence
<<>>=
sims <- as.mcmc(linreg.sim)
# Trace Plots
plot(sims, auto.layout=FALSE, ask=FALSE)
# Autocorrelation plots
autocorr.plot(sims, ask=FALSE)
effectiveSize(sims)
raftery.diag(sims)
@

Here's the frequentest method for an ANOVA
<<>>=
fit.1 <- lm(y~x)
anova(fit.1)
@

to check the value of the slope
<<>>=
beta0 <- sims[,1]
beta1 <- sims[,2]
sigma2 <- sims[,4]

# Probability slope is greater than 0
mean(beta1>0)

# Plot the density of the slope of the line
plot(density(beta1))
@

Plot of regression
<<>>=
plot(x,y, pch=20)
abline(mean(beta0),mean(beta1))
xx <- seq(min(x), max(x), by=0.01)

ans <- beta0+outer(beta1,xx,'*')
lines(xx,apply(ans,2,quantile,.025), col='blue')
lines(xx,apply(ans,2,quantile,.975), col='blue')

# generate a new data set with the first set of
points(xx, beta0[1]+beta1[1]*xx+rnorm(length(xx),0,sqrt(sigma2[1])), pch=20)

@

Homework Plot the prediction interval from the problem in class

generate a normal using  ans each ans and sigma
% section linear_regression (end)
\renewcommand{\dateTaken}{January 29, 2013}
\daysep
\section{Multiple Linear Regression} % (fold)
\label{sec:multiple_linear_regression}
\lstinputlisting[language=SAS]{code/MultipleRegression.sas}
Raftery-lewis diagnostics should be less than 5.

Let the variance increase proportionaly with the $x$s

<<>>=
vo2 <- read.table("data/03vo2.dat", skip=1, col.names=c("ID","Gender","Age1","BMI","MPH","HR","RPE","MaxVO2ML"))
plot(vo2$MPH, vo2$MaxVO2ML)
plot(vo2$BMI, vo2$MaxVO2ML)
plot(vo2$Gender, vo2$MaxVO2ML)
@


DIC Deviance information criteria (calculated in formula~\ref{eq:deviance})
\begin{align} \label{eq:deviance}
    \text{deviance} = -2\log(\mathcal{L})
\end{align}

\renewcommand{\dateTaken}{January 31, 2013}
\daysep
<<>>=
# Using the VO2 Data
N <- nrow(vo2)
mdl <- '
model {
    for( i in 1:N){
        y[i] ~ dnorm(mu[i], prec)
        mu[i] <- b_0 +
                 b_gen*gender[i] +
                 b_bmi*bmi[i] +
                 b_mph*mph[i] +
                 b_hr*hr[i] +
                 b_rpe*rpe[i] +
                 # b_gen_bmi*gen_bmi[i] +
                 # b_gen_mph*gen_mph[i] +
                 # b_gen_hr*gen_hr[i] +
                 # b_gen_rpe*gen_rpe[i] +
                 # b_bmi_mph*bmi_mph[i] +
                 # b_bmi_hr*bmi_hr[i] +
                 # b_bmi_rpe*bmi_rpe[i] +
                 b_mph_hr*mph_hr[i] # +
                 # b_mph_rpe*mph_rpe[i] +
                 # b_hr_rpe*hr_rpe[i]
    }

    b_0   ~ dnorm(0, .00001);
    b_gen ~ dnorm(0, .001);
    b_bmi ~ dnorm(0, .001);
    b_mph ~ dnorm(0, .001);
    b_hr  ~ dnorm(0, .001);
    b_rpe ~ dnorm(0, .001);

    # b_gen_bmi ~ dnorm(0, .001);
    # b_gen_mph ~ dnorm(0, .001);
    # b_gen_hr  ~ dnorm(0, .001);
    # b_gen_rpe ~ dnorm(0, .001);
    # b_bmi_mph ~ dnorm(0, .001);
    # b_bmi_hr  ~ dnorm(0, .001);
    # b_bmi_rpe ~ dnorm(0, .001);
    b_mph_hr  ~ dnorm(0, .001);
    # b_mph_rpe ~ dnorm(0, .001);
    # b_hr_rpe  ~ dnorm(0, .001);

    vr ~ dgamma(2, .05)
    prec <- 1/vr
}
'
writeLines(mdl, "code/multipleRegression.jags")

y <- vo2[,8]
gender <- vo2[,2]
bmi <- vo2[,4]
hr <- vo2[,6]
mph <- vo2[,5]
rpe <- vo2[,7]

# gen_bmi <- gender*bmi
# gen_hr <- gender*hr
# gen_mph <- gender*mph
# gen_rpe <- gender*rpe
# bmi_mph <- bmi*mph
# bmi_hr <- bmi*hr
# bmi_rpe <- bmi*rpe
mph_hr <- mph*hr
# mph_rpe <- mph*rpe
# hr_rpe <- hr*rpe

jags.data <- c(
"N",
"y",
"gender",
"bmi",
"hr",
"mph",
"rpe",
# "gen_bmi",
# "gen_hr",
# "gen_mph",
# "gen_rpe",
# "bmi_mph",
# "bmi_hr" #,
# "bmi_rpe",
"mph_hr" #,
# "mph_rpe",
# "hr_rpe",
)
jags.params <- c(
"b_0",
"b_gen",
"b_bmi",
"b_hr",
"b_mph",
"b_rpe",
# "b_gen_bmi",
# "b_gen_hr",
# "b_gen_mph",
# "b_gen_rpe",
# "b_bmi_mph",
# "b_bmi_hr",
# "b_bmi_rpe",
"b_mph_hr",
# "b_mph_rpe",
"vr"
)

set.seed(1234)
multr.sim <- jags(data=jags.data,
                   parameters.to.save=jags.params,
                   model.file="code/multipleRegression.jags",
                   n.iter=10000, n.burnin=5000, n.chains=1, n.thin=1)
@

\begin{tabular}{cc}
Full & 756.4\\
-hr & 764.4\\
-rpe & 759.2\\
+ all two way interactions & 766.7\\
\end{tabular}

\begin{align*}
    IC= -2\log(\mathcal{L}) + \text{penalty}(\text{parameters})\\
    \text{Deviance} = -2 \log(\mathcal{L})\\
    DIC = -2\log(\mathcal{L}) + pD\\
    pD = \frac{\text{VAR}(\text{deviance})}{2}
\end{align*}
% section multiple_linear_regression (end)
\section{ANOVA} % (fold)
\label{sec:anova}
Cell means model of ANOVA


\begin{tabular}{c|c|c|c}
Treatement 1& Treatement 2 & Treatement 3 & Treatement 4\\
\hline
$y_{11}$ & $y_{21}$ & $y_{31}$ & $y_{41}$ \\
$y_{12}$ & $y_{22}$ & $y_{32}$ & $y_{42}$ \\
$y_{13}$ & $y_{23}$ & $y_{33}$ & $y_{43}$ \\
$y_{14}$ & $y_{24}$ & $y_{34}$ & $y_{44}$ \\
\end{tabular}

so given this data we say that
$y_{1}\sim \mathrm{N}(\mu,\sigma^{2})$

<<tidy=FALSE>>=
anova <- read.table("data/04anova.dat" ,col.names=c("tmt","response","tmt1"))
N <- nrow(anova)
mdl <- 'model {
    # Likelihood
    for (i in 1:N){
        response[i] ~ dnorm(mu[tmt[i]], prec)
    }

    # Create priors for each treatment
    for(i in 1:4){
        mu[i] ~ dnorm(15,.0001)
    }
    prec <- 1/vv
    vv ~ dgamma(1.1,.1)
}
'
writeLines(mdl, "code/ANOVAmodel.jags")
response <- anova$response
tmt <- anova$tmt
@
<<>>=
jags.data <- c("N", "response", 'tmt')
jags.params <- c('mu', 'vv')
anova.sim <- jags(data=jags.data,
                  parameters.to.save=jags.params,
                  model.file="code/ANOVAmodel.jags",
                  n.iter=12000,
                  n.chains=1,
                  n.thin=1
                  )
sims <- as.mcmc(anova.sim)
plot(sims, auto.layout=FALSE, ask=FALSE)
autocorr.plot(sims)
raftery.diag(sims)
effectiveSize(sims)
@

<<tidy=FALSE>>=
mdl <- 'model {
    # Likelihood
    for (i in 1:N){
        response[i] ~ dnorm(mu[tmt[i]], prec[tmt[i]])
    }

    # Create priors for each treatment
    for(i in 1:4){
        mu[i] ~ dnorm(15,.0001)
        prec[i] <- 1/vv[i]
        vv[i] ~ dgamma(1.1,.1)
    }
}
'
writeLines(mdl, "code/ANOVAmodelNCvariance.jags")
@
<<>>=
si
@

Devience is the -2*log likelihood

\subsection{Compare Means} % (fold)
\label{sub:compare_means}
We don't care about multiple test becasue we are not under the constraint of a null hypothisis
<<>>=
diff12 <- sims[,2] - sims[,3]
mean( dif12 > 0 )
dif13 <- sims[,2] - sims[,4]
dif14 <- sims[,2] - sims[,5]
dif23 <- sims[,3] - sims[,4]
dif24 <- sims[,3] - sims[,5]
dif34 <- sims[,4] - sims[,5]
mean(dif13>0)
mean(dif14>0)
mean(dif23>0)
mean(dif24>0)
mean(dif34>0)
@
% subsection compare_means (end)

Two by three factorial design.
<<>>=
twoway <- read.table("data/05twoway.dat", col.names=c("block","seedType","inoculate","yield"))
@
Using cell means method

Table should be rotated
\begin{tabular}{c|cc|c}
 & a & b & \\
 \hline
dea & $\mu_{11}$ & $\mu_{12}$ & $\mu_{1\cdot}$\\
con & $\mu_{21}$ & $\mu_{22}$ & $\mu_{2\cdot}$\\
liv & $\mu_{31}$ & $\mu_{22}$ & $\mu_{3\cdot}$\\
\hline
    & $\mu_{\cdot 1}$ & $\mu_{\cdot 2}$ & \\
\end{tabular}

Degrees of freedom is 5, one for type, two for inoculate, and two for interaction

<<tidy=FALSE>>=
y <- as.numeric(twoway$seedType)

mdl <- '
model {
    for (i in 1:24) {
        yield[i] ~ dnorm(mu[ncult[i],ninoc[i]], 1/vv)
    }

    for (i in 1:2) {
        for (j in 1:3) {
            mu[i,j] ~ dnorm(0, .000001)
        }
    }

    vv ~ dgamma(1.5, .1)
}
'

writeLines(mdl, 'code/CultModel.jags')

yield <- twoway$yield
ncult <- as.numeric(twoway$seedType)
ninoc <- as.numeric(twoway$inoculate)

jags.data <- c('yield', 'ncult', 'ninoc')
jags.params <- c('mu','vv')

innits <- function(){list('mu'= matrix(0,2,3), 'vv', 15)}

cult1.jags <- jags( data=jags.data,
                    # inits=innits,
                    parameters.to.save=jags.params,
                    model.file='code/CultModel.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
cult1.sim <- as.mcmc(cult1.jags)
@

Effects model and cell means model are two different ways to do analysis of variance. BYU was a pioneer of the cell means model.

\subsection{ANOVA Marginal Means} % (fold)
\label{sub:anova_marginal_means}

marginals are calculated as $\mu_{1\cdot} = \frac{\mu_{11} + \mu_{12} + \mu_{13}}{3}$
its just a mean of the means
<<>>=
mu1dot <- (cult1.sim[,2] + cult1.sim[,4] + cult1.sim[,6])/3
mu2dot <- (cult1.sim[,3] + cult1.sim[,5] + cult1.sim[,7])/3

plot(mu1dot-mu2dot)
mean(mu2dot-mu1dot>0)

mudot1 <- (cult1.sim[,2] + cult1.sim[,3])/2
mudot2 <- (cult1.sim[,4] + cult1.sim[,5])/2
mudot3 <- (cult1.sim[,6] + cult1.sim[,7])/2

mean(mudot1 - mudot2 < 0) # Probability mu_dot2 is bigger than mu_dot1
mean(mudot3 - mudot2 > 0) # Probability mu_dot3 is bigger than mu_dot2
@
% subsection anova_marginal_means (end)
\subsection{ANOVA Interactions} % (fold)
\label{sub:anova_interactions}
interactions are tested in four cell groups the formula is
\begin{align*}
    \mu_{11}-\mu_{21} = \mu_{12}-\mu_{22}\\
    \mu_{11}-\mu_{21} - \mu_{12}+\mu_{22} = 0
\end{align*}


<<>>=
int1 <- cult1.sim[,2] - cult1.sim[,4] - cult1.sim[,3] - cult1.sim[,4]
int2 <- cult1.sim[,4] - cult1.sim[,6] - cult1.sim[,5] + cult1.sim[,7]
@
% subsection anova_interactions (end)

Finally, compare to seed types that with the best inoculate, does it make sense to pay for the better seed or does the $\mu_{11}-\mu_{21} = \mu_{12}-\mu_{22}$ come from the inoculate
<<>>=
se1 <- cult1.sim[,6] - cult1.sim[,7]
mean(se1<0)
@
\subsection{Linear Algebra Solution} % (fold)
\label{sec:linear_algebra_solution}
% \begin{align*}
%     \begin{bmatrix}
%         27.4\\
%         29.7\\
%         34.5\\
%     \end{bmatrix}
%      = 
%      \begin{bmatrix}
%      \end{bmatrix}
% \end{align*}

\[
\underline{\hat{\mu}} = (W' W)^{-1} W' \underline{y}
\]
Least Squares Solution
<<>>=
w  <- rbind(diag(1,6),diag(1,6),diag(1,6),diag(1,6))
muhat <- solve(t(w) %*% w) %*% t(w) %*% twoway$yield
@

\begin{align*}
    y=W\mu\\
    y=WI\mu\\
    y= \underbrace{WA^{-1}}_{X} \underbrace{A\mu}_{\beta}
\end{align*}


\begin{align*}
\begin{bmatrix}
    \frac{1}{6} & \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}\\
    \frac{1}{6} & \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}& \frac{1}{6}\\
    \frac{1}{2} & -\frac{1}{2}& 0& \frac{1}{2}& -\frac{1}{2}& 0\\
    0&\frac{1}{2} & -\frac{1}{2}& 0&\frac{1}{2}& -\frac{1}{2}\\
    1 & -1 & 0 & -1 & 1 & 0\\
    0 & 1 & -1 & 0 & -1 & 1\\
\end{bmatrix}
\begin{bmatrix}
    \mu_{11}\\
    \mu_{12}\\
    \mu_{13}\\
    \mu_{21}\\
    \mu_{22}\\
    \mu_{23}\\
\end{bmatrix}
=
\begin{bmatrix}
    \mu_{\cdot\cdot}\\
    \mu_{1\cdot} - \mu_{2\cdot}\\
    \mu_{\cdot1} - \mu_{\cdot2}\\
    \mu_{\cdot2} - \mu_{\cdot3}\\
    \mu_{11}  - \mu_{12} - \mu_{21}+ \mu_{22}\\
    \mu_{12}  - \mu_{13} - \mu_{22}+ \mu_{23}\\
\end{bmatrix}
\end{align*}
% subsection linear_algebra_solution (end)
% section anova (end)
\renewcommand{\dateTaken}{February 12, 2013}
\daysep
<<eval=FALSE>>=
dev <- function(x) {
    fp <- length(y) * log(2* pi)
    sp <- 
}
@
\renewcommand{\dateTaken}{February 14, 2013}
\daysep
\section{Analysis of Covariance ANCOVA} % (fold)
\label{sec:analysis_of_covariance_ancova}

Join Me 777-129-200

<<>>=
ac <- read.table("data/06ancova.dat", header=TRUE)
@

First thing you do for analysis is plot the data
<<>>=
plot(ac$speed,ac$scrap,pch=ac$lines)
@

Let's try and come up with a model
\begin{align}
y_{i j} = \beta_{0} + \beta_{1}\cdot\text{Line} + \beta_{2}\cdot\text{Speed} + \beta_{3}\cdot \text{Line} \cdot \text{Speed}
\end{align}

Or we could do this
\begin{align*}
    y_{i j} = \beta_{0 i} + \beta_{1 i}\cdot\text{Speed}
\end{align*}

<<tidy=FALSE>>=
mdl <- 'model {
    for(i in 1:27) {
        scrap[i] ~ dnorm(mu[i], prec);
        mu[i] <- b_0[line[i]] + b_1[line[i]]*speed[i];
    }

    for (i in 1:2) {
        b_0[i] ~ dnorm(30, .001);
        b_1[i] ~ dnorm(0, .01);
    }

    vr ~ dgamma(1.5, .0125);
    prec <- 1/vr;
}
'
writeLines(mdl, 'code/ANCOVAModel.jags')
@
prior for beta not covers 30*100

<<>>=
speed <- ac$speed
line <- ac$line
scrap <- ac$scrap
jags.data <- c('speed', 'line', 'scrap')
jags.params <- c('b_0','b_1', 'vr')
ancova.jags <- jags( data=jags.data,
                    # inits=innits,
                    parameters.to.save=jags.params,
                    model.file='code/ANCOVAModel.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
ancova.jags
@

Diagnostics
<<ANCOVA_Diagnostics,out.width='.19\\textwidth'>>=
ancova.sim <- as.mcmc(ancova.jags)
plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
autocorr.plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
raftery.diag(ancova.sim)
@


Test the probability that the $\beta_{1\cdot}$ are different
<<>>=
b11 <- ancova.sim[,3]
b12 <- ancova.sim[,4]
slopdif <- b11 - b12
mean(slopdif < 0)
mean(slopdif > 0)
@

Try a new model without the different $\beta_{1}$ then compare the DIC

<<tidy=FALSE>>=
mdl <- 'model {
    for(i in 1:27) {
        scrap[i] ~ dnorm(mu[i], prec);
        mu[i] <- b_0[line[i]] + b_1*speed[i];
    }

    b_1 ~ dnorm(0, .01);
    for (i in 1:2) {
        b_0[i] ~ dnorm(30, .001);
    }

    vr ~ dgamma(1.5, .0125);
    prec <- 1/vr;
}
'
writeLines(mdl, 'code/ANCOVAModel_1.jags')
@
prior for beta not covers

<<>>=
speed <- ac$speed
line <- ac$line
scrap <- ac$scrap
jags.data <- c('speed', 'line', 'scrap')
jags.params <- c('b_0','b_1', 'vr')
ancova1.jags <- jags( data=jags.data,
                    # inits=innits,
                    parameters.to.save=jags.params,
                    model.file='code/ANCOVAModel_1.jags',
                    n.iter=12000, n.burnin=2000,
                    n.chains=1, n.thin=1)
ancova1.jags
@

Diagnostics
<<ANCOVA1_Diagnostics,out.width='.19\\textwidth'>>=
ancova1.sim <- as.mcmc(ancova1.jags)
# plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
# autocorr.plot(ancova.sim, auto.layout=FALSE, ask=FALSE)
# raftery.diag(ancova.sim)
@

Lets test differences in line by testing difference in intercept
<<ANCOVA1_Line_Diff>>=
b_0_1 <- ancova1.sim[,1]
b_0_2 <- ancova1.sim[,2]
line_diff <- b_0_1 - b_0_2
mean(line_diff < 0)
mean(line_diff > 0)
plot(density(line_diff), col='blue')
@

<<>>=
plot(ac$speed, ac$scrap, pch=ac$lines)
abline(82.2, 1.22, col='blue')
abline(13.7, 1.3, col='red')

b_0_1 <- ancova.sim[,1]
b_0_2 <- ancova.sim[,2]
b_1_1 <- ancova.sim[,3]
b_1_2 <- ancova.sim[,4]

y1_300 <- 300*b_1_1 + b_0_1
y2_300 <- 300*b_1_2 + b_0_2
diff_300 <- y1_300 - y2_300
mean(diff_300 > 0)
@
% section analysis_of_covariance_ancova (end)
\renewcommand{\dateTaken}{February 21, 2013}
\daysep
\begin{tabular}{ccccccc}
   &   &    & B  &    &    & \\
   &   &  1 &  2 &  3 &  4 & \\
   & 1 &  $\mu_{11}$ &  $\mu_{12}$ &  $\mu_{13}$ &  $mu_{14}$ &  $\mu_{1\bullet}$\\ %$\frac{\mu_{11} +  \mu_{12} +  \mu_{13} +  mu_{14}}{4}=\mu_{1\bullet}$\\
 A & 2 &  $\mu_{21}$ &  $\mu_{22}$ &  $\mu_{23}$ &  $mu_{24}$ &  $\mu_{2\bullet}$\\ %$\frac{\mu_{21} +  \mu_{22} +  \mu_{23} +  mu_{24}}{4}=\mu_{2\bullet}$\\
   & 3 &  $\mu_{31}$ &  $\mu_{32}$ &  $\mu_{33}$ &  $mu_{34}$ &  $\mu_{3\bullet}$\\ %$\frac{\mu_{31} +  \mu_{32} +  \mu_{33} +  mu_{34}}{4}=\mu_{3\bullet}$\\
   & 4 &  $\mu_{41}$ &  $\mu_{42}$ &  $\mu_{43}$ &  $mu_{44}$ &  $\mu_{4\bullet}$\\ %$\frac{\mu_{41} +  \mu_{42} +  \mu_{43} +  mu_{44}}{4}=\mu_{4\bullet}$\\
   % &   & $\frac{\mu_{11} +  \mu_{21} +  \mu_{31} +  mu_{41}}{4}=\mu_{\bullet 1}$
   %     & $\frac{\mu_{12} +  \mu_{22} +  \mu_{32} +  mu_{42}}{4}=\mu_{\bullet 2}$
   %     & $\frac{\mu_{13} +  \mu_{23} +  \mu_{33} +  mu_{43}}{4}=\mu_{\bullet 3}$
   %     & $\frac{\mu_{14} +  \mu_{24} +  \mu_{34} +  mu_{44}}{4}=\mu_{\bullet 4}$\\
\end{tabular}

\section{Multiple Sources of Variation} % (fold)
\label{sec:multiple_sources_of_variation}
\begin{align*}
    \underline{y}= X\underline{\beta} + \underline{\varepsilon}\\
    \varepsilon \sim \mathcal{N}(\underline{0}, \sigma^{2 \underline{T}})\\
    \underline{y} = X \underline{\beta} + Z \underline{u} + e\\
    e\sim \mathcal{N}(\underline{0}, R)\\
    \underline{u} \sim \mathcal{N}(\underline{0}, G)\\
\end{align*}

\begin{align*}
    \expv(Y)= \expv(X \beta + Z u + e) = \expv(X \beta) + \expv(Z u) + \expv(e)= X \beta\\
    \var(Y) = \var(X \beta + Z u + e) = \var( Z u + e)
\end{align*}
If we assume that $u$ and $e$ are independent then we can move forward easily

$V(A\underline{z})=A V(z) A'$
$A V(z) A'$ is a $p \times p$ matrix
\begin{align*}
    \var( Z u + e)= V(Z\underline{u}) + \var(\underline{e}) = Z \var(\underline{u}) Z' + V(\underline{e} = Z G Z' + R
\end{align*}
% section multiple_sources_of_variation (end)
\end{document}
